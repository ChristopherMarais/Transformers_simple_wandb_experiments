{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a96a75d-3dab-421b-91cb-64fc77378b0e",
   "metadata": {},
   "source": [
    "# Apply and Test\n",
    "\n",
    "This script shows how the model can be applied when it has already been trained. It also shows how each target can be classified from the SDGs and an embedding model.\n",
    "\n",
    "Further this script shows how these classification models can be tested and evaluated on unseen data to get an estiamte of the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7deb845-3059-4a9a-a531-db0d3cf04e91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:11:26.748545Z",
     "iopub.status.busy": "2022-08-25T18:11:26.747536Z",
     "iopub.status.idle": "2022-08-25T18:11:26.763549Z",
     "shell.execute_reply": "2022-08-25T18:11:26.762546Z",
     "shell.execute_reply.started": "2022-08-25T18:11:26.748545Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from simpletransformers.classification import MultiLabelClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "585f395c-dd77-4b78-8fa8-375e1a3fd73a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T17:55:27.545901Z",
     "iopub.status.busy": "2022-08-25T17:55:27.545901Z",
     "iopub.status.idle": "2022-08-25T17:55:27.563884Z",
     "shell.execute_reply": "2022-08-25T17:55:27.562211Z",
     "shell.execute_reply.started": "2022-08-25T17:55:27.545901Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see GPU avaialability\n",
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1c175-6bf5-4ba8-bcfd-5a431c7c52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unseen data \n",
    "data = pd.read_csv('OneHot_Combined_cln_utf8.tsv', sep='\\t')\n",
    "data = data[data['source']=='SASDG_Hub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05e70d3f-3cd3-460d-9286-e8e552754cb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T17:55:27.680225Z",
     "iopub.status.busy": "2022-08-25T17:55:27.680225Z",
     "iopub.status.idle": "2022-08-25T17:55:28.421261Z",
     "shell.execute_reply": "2022-08-25T17:55:28.421261Z",
     "shell.execute_reply.started": "2022-08-25T17:55:27.680225Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to apply model to text\n",
    "def classify_sdg(text_lst):\n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    # import model from path (this path is the directory with all the model files)\n",
    "    sdg_model = MultiLabelClassificationModel(\n",
    "            \"xlnet\",\n",
    "            \"outputs/best_model/\",   #os.getcwd()+\"\\\\outputs\\\\best_model\\\\\", #C:\\Users\\GCM\\Desktop\\GIT_REPOS\\Transformers_simple_wandb_experiments\\SASDGHUB\\\n",
    "            num_labels=17,\n",
    "            use_cuda=cuda_available,\n",
    "            )\n",
    "    predictions, raw_outputs = sdg_model.predict(text_lst)\n",
    "    return(predictions, raw_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6258364b-81b5-4f79-a7a2-cca64b7e81a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T17:18:05.388333Z",
     "iopub.status.busy": "2022-08-25T17:18:05.388333Z",
     "iopub.status.idle": "2022-08-25T17:18:14.330398Z",
     "shell.execute_reply": "2022-08-25T17:18:14.329396Z",
     "shell.execute_reply.started": "2022-08-25T17:18:05.388333Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2a9efa911440dfa9804cd196406b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e6573e4ad3429fa9ea475e4c855a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions with the model\n",
    "# raw outputs are the probabilities of each class\n",
    "predictions, raw_outputs = model.predict([\"sustainable development goal 3 Good Health and Well-being\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31510a1c-eaa1-424a-95dd-fae0acf5e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make target predictions\n",
    "# target prediction requires SDG classification to already be done\n",
    "# function to classify targets\n",
    "def classify_sdg_target(text_lst, \n",
    "                        target_data_path='Targets.csv',\n",
    "                        run_isomap=True, # run faster for multiple samples otherwise (target_data_path required when this is False) (do not use when only one sample)\n",
    "                        target_embedding_reduced_path=None, # load a previously calculated and reduced embedding for the targets 'outputs/targets_embedded_reduced_gpt2_2D.csv'\n",
    "                        isomap_dims = 2,\n",
    "                        isomap_neigbors = 5, # has to be <= len(text_lst) a.k.a n_samples\n",
    "                        pre_trained_model_type='gpt2', \n",
    "                        pre_trained_model_name='gpt2',\n",
    "                        target_threshold_val=0.5):\n",
    "\n",
    "    # define and load model from hugging face\n",
    "    model_args = ModelArgs(max_seq_length=1024)\n",
    "    # model import\n",
    "    model = RepresentationModel(\n",
    "        pre_trained_model_type,\n",
    "        pre_trained_model_name, #gpt2 , gpt2-large\n",
    "        args=model_args,\n",
    "    )\n",
    "    \n",
    "    # classify sdg of text\n",
    "    sdg_predictions, sdg_raw_outputs = classify_sdg(text_lst) ########################## remove and replace with already having sdg data\n",
    "    \n",
    "    \n",
    "    # get embeddings of text\n",
    "    word_embeddings = model.encode_sentences(text_lst, combine_strategy=\"mean\")\n",
    "    \n",
    "    \n",
    "    # ISOMAP\n",
    "    if run_isomap==True: \n",
    "        # reduce isomap_neigbors to fit the number of samples\n",
    "        n_samples = len(text_lst)\n",
    "        if n_samples < isomap_neigbors:\n",
    "            isomap_neigbors = np.max([1,n_samples-1])\n",
    "            print('Reduced isomap_n_neigbors to: ', isomap_neigbors)\n",
    "\n",
    "        # reduce dimensions of embeddings to 2 (can be reduced to higher dimensions)\n",
    "        isomap = Isomap(n_components=isomap_dims, n_neighbors=isomap_neigbors-1) # input is an array with samples x features\n",
    "        word_embeddings_transformed = isomap.fit_transform(word_embeddings)\n",
    "\n",
    "        if target_embedding_reduced_path == None:\n",
    "            # load pre-calculated embeddings\n",
    "            target_df = pd.read_csv(target_data_path, sep=';')\n",
    "            # get sentence list from target data\n",
    "            target_sentence_list = target_df['text'].tolist()\n",
    "            # get embeddings of targets\n",
    "            target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "            target_embeddings_transformed = isomap.fit_transform(target_embeddings)\n",
    "\n",
    "            # add labels to reduced embeddings\n",
    "            target_trans_df = pd.DataFrame(target_embeddings_transformed)\n",
    "            target_trans_df['target'] = target_df['target']\n",
    "            target_trans_df['sdg'] = target_df['sdg']\n",
    "            target_trans_df.to_csv('outputs/targets_embedded_reduced_'+pre_trained_model_name+'_'+str(isomap_dims)+'D.csv', index=False)\n",
    "\n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "            \n",
    "        else:\n",
    "            # import reduced embedding of targets\n",
    "            target_trans_df = pd.read_csv(target_embedding_reduced_path, sep=',')\n",
    "            \n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "\n",
    "        # plot embeddings if they are 2D\n",
    "        if isomap_dims ==2:\n",
    "            trans_df = pd.DataFrame(word_embeddings_transformed)\n",
    "            trans_df['target'] = target_df['target']\n",
    "            trans_df['sdg'] = target_df['sdg']\n",
    "            trans_df.plot.scatter(0,1,c='sdg', colormap='viridis') # colour by sdg\n",
    "            plt.title('Isomap 2D plot of text embedding')\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        # load pre-calculated embeddings\n",
    "        target_df = pd.read_csv(target_data_path, sep=';')\n",
    "        # get sentence list from target data\n",
    "        target_sentence_list = target_df['text'].tolist()\n",
    "        # get embeddings of targets\n",
    "        target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "        \n",
    "        # define source and target for KNN\n",
    "        Y = pd.DataFrame(target_embeddings)\n",
    "        Y['target'] = target_df['target']\n",
    "        Y['sdg'] = target_df['sdg']\n",
    "        X = pd.DataFrame(word_embeddings)\n",
    "        idx_ar = np.array(range(0,17), np.int64)\n",
    "        sdg_label_lst = []\n",
    "        for row in (np.array(sdg_predictions)==1):\n",
    "            sdg_label_lst.append(idx_ar[row])\n",
    "        X['sdg'] = sdg_label_lst\n",
    "        sdg_prob_lst = []\n",
    "        for row in sdg_raw_outputs:\n",
    "            sdg_prob_lst.append(row)\n",
    "        X['sdg_probability'] = sdg_prob_lst\n",
    "    \n",
    "    # use cosine similarity Kmeans variant to classify targets\n",
    "    # define final results table\n",
    "    results_df = pd.DataFrame()\n",
    "    results_df['text'] = text_lst\n",
    "    results_df['sdg'] = X['sdg']\n",
    "    results_df['sdg_probability'] = X['sdg_probability']\n",
    "    # calculate pairwise cosine similarity between targets and text list\n",
    "    similarity_ar = cosine_similarity(X.loc[:, ~X.columns.isin(['sdg', 'sdg_probability'])], \n",
    "                                  Y.loc[:, ~Y.columns.isin(['sdg', 'target'])],\n",
    "                                 )\n",
    "    results_df['target_similarity'] = similarity_ar.tolist()\n",
    "    # select targets on distance sdg and threshold\n",
    "    targets_ar = np.array(Y['target'])\n",
    "    targets_full_ar = np.tile(targets_ar, (len(text_lst), 1)) \n",
    "    sdg_ar = np.array(Y['sdg'])\n",
    "    sdg_full_ar = np.tile(sdg_ar, (len(text_lst), 1))\n",
    "    sdg_select_ar = np.array(X['sdg'])\n",
    "    \n",
    "    # select classified SDGs\n",
    "    sdg_onehot_lst = []\n",
    "    for i in range(len(sdg_full_ar)):\n",
    "        isin_ar = np.isin(sdg_full_ar[i], sdg_select_ar[i])\n",
    "        sdg_onehot_lst.append(isin_ar)\n",
    "    sdg_onehot_ar = np.vstack(sdg_onehot_lst)\n",
    "    target_onehot_ar = (similarity_ar>=target_threshold_val)*sdg_onehot_ar\n",
    "    target_label_lst = (targets_full_ar*target_onehot_ar).tolist()\n",
    "    results_df['target'] = [[ele for ele in sub if ele != ''] for sub in target_label_lst]\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef205b4-ffa7-489f-8201-a5336409d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply classifications within wandb to capture data\n",
    "# log performance and outputs to wandb\n",
    "# print performance metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
