{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee564013-69a3-4853-98fe-6c2a00844ebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T22:58:55.309139Z",
     "iopub.status.busy": "2022-08-18T22:58:55.309139Z",
     "iopub.status.idle": "2022-08-18T22:58:59.872733Z",
     "shell.execute_reply": "2022-08-18T22:58:59.871724Z",
     "shell.execute_reply.started": "2022-08-18T22:58:55.309139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43217174-039a-41fa-9ab1-0441bc061dd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T22:58:59.874225Z",
     "iopub.status.busy": "2022-08-18T22:58:59.874225Z",
     "iopub.status.idle": "2022-08-18T22:58:59.918725Z",
     "shell.execute_reply": "2022-08-18T22:58:59.918224Z",
     "shell.execute_reply.started": "2022-08-18T22:58:59.874225Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see GPU avaialability\n",
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2410685e-4550-4080-aa6b-74b1cce94dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T22:58:59.919725Z",
     "iopub.status.busy": "2022-08-18T22:58:59.919224Z",
     "iopub.status.idle": "2022-08-18T22:59:01.111130Z",
     "shell.execute_reply": "2022-08-18T22:59:01.110628Z",
     "shell.execute_reply.started": "2022-08-18T22:58:59.919725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% PER CLASS:\n",
      "\n",
      " SDG1      5.354267\n",
      "SDG2      5.152979\n",
      "SDG3      7.447665\n",
      "SDG4      4.388084\n",
      "SDG5      4.186795\n",
      "SDG6      5.314010\n",
      "SDG7      5.756844\n",
      "SDG8      6.400966\n",
      "SDG9      5.636071\n",
      "SDG10     5.152979\n",
      "SDG11     5.354267\n",
      "SDG12     5.112721\n",
      "SDG13    10.265700\n",
      "SDG14     4.830918\n",
      "SDG15     5.676329\n",
      "SDG16     4.951691\n",
      "SDG17     9.017713\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import data\n",
    "data = pd.read_csv('OneHot_Combined_cln_utf8.tsv', sep='\\t')\n",
    "data = data.iloc[-1000:,:]\n",
    "\n",
    "# reformat data\n",
    "sdg_lst = ['SDG1','SDG2','SDG3','SDG4','SDG5','SDG6','SDG7','SDG8','SDG9','SDG10','SDG11','SDG12','SDG13','SDG14','SDG15','SDG16','SDG17']\n",
    "data['y'] = data[sdg_lst].values.tolist()\n",
    "y = data['y']\n",
    "X = data['abstract']\n",
    "\n",
    "# plot ratio of data\n",
    "class_weight = (data[sdg_lst].sum()/ data[sdg_lst].sum().sum())\n",
    "print('% PER CLASS:\\n\\n', class_weight*100)\n",
    "data[sdg_lst].sum().plot.bar()\n",
    "\n",
    "# split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# add data to dataframes\n",
    "train_df = pd.DataFrame()\n",
    "train_df['text'] = X_train\n",
    "train_df['labels'] = y_train\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "eval_df = pd.DataFrame()\n",
    "eval_df['text'] = X_val\n",
    "eval_df['labels'] = y_val\n",
    "eval_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# get number of classes\n",
    "label_count = len(sdg_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab116e8c-6382-48c5-b23d-bc7b3553c389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T22:59:01.112630Z",
     "iopub.status.busy": "2022-08-18T22:59:01.112630Z",
     "iopub.status.idle": "2022-08-18T22:59:03.867987Z",
     "shell.execute_reply": "2022-08-18T22:59:03.867484Z",
     "shell.execute_reply.started": "2022-08-18T22:59:01.112630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: mymmnvmj\n",
      "Sweep URL: https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\n"
     ]
    }
   ],
   "source": [
    "# parameter optimisation\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # bayes, grid, random\n",
    "    \"metric\": {\"name\": \"train_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"min\": 1, \"max\": 10},\n",
    "        \"learning_rate\": {\"min\": 5e-5, \"max\": 4e-4},\n",
    "        \"train_batch_size\":{\"min\": 5, \"max\": 15},\n",
    "        \"eval_batch_size\":{\"min\": 5, \"max\": 15},\n",
    "        \"warmup_steps\":{\"min\": 50, \"max\": 500},\n",
    "        \"weight_decay\":{\"min\": 0.01, \"max\": 0.1},\n",
    "        \"logging_steps\":{\"values\": [2, 5, 10]},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, entity='sasdghub', project=\"sasdghub_ml_classify\")\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eda9b5-fdb4-4cc5-88d7-ad52fee899a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T22:59:03.868985Z",
     "iopub.status.busy": "2022-08-18T22:59:03.868985Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.agents.pyagent:Starting sweep agent: entity=None, project=None, count=None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 89qa21om with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002677062184906668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 452\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.02766290793707818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m (\u001b[33msasdghub\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_185906-89qa21om</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/89qa21om\" target=\"_blank\">solar-sweep-1</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905b4087a50a42bb92844b947d7fd38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_xlnet_128_0_2\n",
      "C:\\Users\\GCM\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641cc14518914fa8af3b7a9cd28fe880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4c21ce13e843509d057e4bc289c105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8680086a19d421189b6eb96ce1a7dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef337d681ed74dcab05ea3c7b856496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ebb88b92c04f15a4eee1b7d727e3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e347b0d8f9244619933bec349c2e9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb40dc29622544b496a90b0f5e6aca46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9199903ab45844a1b1343da57d195991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a349290f344680a1d207dfb15cbcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e5b31872e648f49dd2e76dee89337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeec7eea06bf488189c13b795a5ab166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213ae71c76b849828687663b835c3fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a397d216fede4af38ae76da8d0554d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b1a15cd45b4900be20d08d122ee9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951122b816444b9daec919fbe91e9091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5785617b5a4b38a9532a1502e29c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddf310f8b4b4c7bbdf4d8271b5e45fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84c2f04afc34aa3ac2c7a087780baae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 9:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba34cc7622b4449ba6bbeb3ce3a8668c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n",
      "INFO:simpletransformers.classification.classification_model: Training of xlnet model complete. Saved to outputs/.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a33cf5cd824ac593e2646f2c335b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79761f5c75ca445b96ff7fa3860fdb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'LRAP': 0.3409374308569037, 'eval_loss': 0.4029125382502874}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LRAP</td><td>▃██▅▁▁▁▁▁</td></tr><tr><td>Training loss</td><td>█▅▅▃▄▄▂▂▂▃▄▄▂▁▂▄▅▄▃▃▅▆▆▃▄█▂▃▄▄▅▅▆▆▃▇▅▃▇▃</td></tr><tr><td>eval_loss</td><td>▄▂▁▄█▆▆▆▆</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train_loss</td><td>▃▁▁▃▂▇▃█▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LRAP</td><td>0.34094</td></tr><tr><td>Training loss</td><td>0.294</td></tr><tr><td>eval_loss</td><td>0.40291</td></tr><tr><td>global_step</td><td>792</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.31312</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-sweep-1</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/89qa21om\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/89qa21om</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_185906-89qa21om\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jdkmaqyr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00019095905200630577\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.05697227412475562\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_190433-jdkmaqyr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/jdkmaqyr\" target=\"_blank\">neat-sweep-2</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dffe411b10c4f218cc193d0b11defe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_xlnet_128_0_2\n",
      "C:\\Users\\GCM\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2da25d3cc04b9496aad07286521099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ac23bab9304e7c909245ae81a31c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc7311bd5724c64a0cb63e6ce438fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c796df3208be45ecab1cbc33027137b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a2594da44a4b6e96a6320f649793bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360c51c9ef054c66be80b3e562b19ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797b7b7570e544118cc5a0fc4a3ad039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LRAP</td><td>▁█</td></tr><tr><td>Training loss</td><td>█▇▆▄▅▅▄▄▃▃▅▃▄▂▃▃▂▂▄▂▃▂▃▅▂▄▄▁▃▇▃▂▄▅▂▄▄▄▃▂</td></tr><tr><td>eval_loss</td><td>█▁</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██████████▇▇▇▇▇▇▇</td></tr><tr><td>train_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LRAP</td><td>0.56946</td></tr><tr><td>Training loss</td><td>0.28718</td></tr><tr><td>eval_loss</td><td>0.35377</td></tr><tr><td>global_step</td><td>210</td></tr><tr><td>lr</td><td>0.00017</td></tr><tr><td>train_loss</td><td>0.38032</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">neat-sweep-2</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/jdkmaqyr\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/jdkmaqyr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_190433-jdkmaqyr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run jdkmaqyr errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 775793536 vs 775793424')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jdkmaqyr errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 775793536 vs 775793424')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 99w17iql with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00033899090731834405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 179\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.04958825279796908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_190643-99w17iql</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/99w17iql\" target=\"_blank\">divine-sweep-3</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec1265a99ac4e2e99399be1fbe7e31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_xlnet_128_0_2\n",
      "C:\\Users\\GCM\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08052ac60fe349c2992931bfd08d0d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088cf0e2cfd24946bf4fd08fe3eefb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 8:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab3c4ab825344b083df87fd3b4e0197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▆▄▃▇▁▁▂▄▂▄▄▁▂▂</td></tr><tr><td>global_step</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>lr</td><td>▁▁▂▂▃▄▄▄▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>0.35347</td></tr><tr><td>global_step</td><td>75</td></tr><tr><td>lr</td><td>0.00014</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">divine-sweep-3</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/99w17iql\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/99w17iql</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_190643-99w17iql\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 99w17iql errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 31680 vs 31632')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 99w17iql errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 31680 vs 31632')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cq2l7toq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00024476393486374214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.012676189088912325\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_190725-cq2l7toq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/cq2l7toq\" target=\"_blank\">sunny-sweep-4</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7eebdfcbb149e2b41f32071e1462c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4506d7648ff4dfe881a9b2d8207f681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956c089a8831443fa76c5c284d51511c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 9:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0affdcc091472a80d8307537e0292e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562e824755b64808b7f15734f8e0610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LRAP</td><td>▁</td></tr><tr><td>Training loss</td><td>█▅▅▄▂▂▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>global_step</td><td>▁▂▃▅▆▇██</td></tr><tr><td>lr</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LRAP</td><td>0.42249</td></tr><tr><td>Training loss</td><td>0.29172</td></tr><tr><td>eval_loss</td><td>0.3644</td></tr><tr><td>global_step</td><td>70</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train_loss</td><td>0.29172</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sunny-sweep-4</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/cq2l7toq\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/cq2l7toq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_190725-cq2l7toq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run cq2l7toq errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 929228352 vs 929228240')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run cq2l7toq errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 929228352 vs 929228240')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d89711tp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00039183996239433295\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01647788590429503\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_190946-d89711tp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/d89711tp\" target=\"_blank\">robust-sweep-5</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171bdb4cb98d4deb84737bb7f8123047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_xlnet_128_0_2\n",
      "C:\\Users\\GCM\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca356dfa9664a22aea17eeea0bbf04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924a8d94243344f19597194741d03db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cf7064e9b44eada04f5f003cf57161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f55310f646546539a72028f2b21acae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LRAP</td><td>▁</td></tr><tr><td>Training loss</td><td>███▆▆▅▄▄▂▄▄▁▅▄▆▂▁█▆▁▂▅▂▁▂▅▅▃▄▃▅▇▃▂▂▃▂▃▆▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LRAP</td><td>0.34145</td></tr><tr><td>Training loss</td><td>0.26403</td></tr><tr><td>eval_loss</td><td>0.40617</td></tr><tr><td>global_step</td><td>140</td></tr><tr><td>lr</td><td>0.00015</td></tr><tr><td>train_loss</td><td>0.26403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">robust-sweep-5</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/d89711tp\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/d89711tp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_190946-d89711tp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run d89711tp errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 929228352 vs 929228240')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run d89711tp errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 929228352 vs 929228240')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4a1ucskv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.949713486806392e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.028280170972710458\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191131-4a1ucskv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/4a1ucskv\" target=\"_blank\">happy-sweep-6</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986ca8b2e9d240d5b27d0a1ce4f81f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_xlnet_128_0_2\n",
      "C:\\Users\\GCM\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5faa1a31ad4fa6865d12776343f8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee56b8a6d0a45049891265705b46d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 7:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7c8b2032a84bc5a0e746b54d12ae8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▇▆▅▄▃▃▁▁</td></tr><tr><td>global_step</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>lr</td><td>▁▂▃▄▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>0.35169</td></tr><tr><td>global_step</td><td>45</td></tr><tr><td>lr</td><td>2e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">happy-sweep-6</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/4a1ucskv\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/4a1ucskv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191131-4a1ucskv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 4a1ucskv errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 31680 vs 31632')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4a1ucskv errored: RuntimeError('[enforce fail at C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\caffe2\\\\serialize\\\\inline_container.cc:319] . unexpected pos 31680 vs 31632')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: id5xh67o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00019479768956926104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.011520854050774108\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191213-id5xh67o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/id5xh67o\" target=\"_blank\">northern-sweep-7</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ac4837dae943f886e3d8e08fa5d75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_xlnet_128_0_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44536d91343246ca9e142431a4528b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5074536882264c6187ad39b32bfa30cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 8:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf76ccefb2b4d7da39e47d4b526857d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">northern-sweep-7</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/id5xh67o\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/id5xh67o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191213-id5xh67o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run id5xh67o errored: RuntimeError('CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run id5xh67o errored: RuntimeError('CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lmonqxn2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002664844121116928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.03380516513986738\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191239-lmonqxn2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/lmonqxn2\" target=\"_blank\">colorful-sweep-8</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4929e3f118ca4d9d972745c803e4c87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">colorful-sweep-8</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/lmonqxn2\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/lmonqxn2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191239-lmonqxn2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run lmonqxn2 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run lmonqxn2 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ucbas7xx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003042795123965731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.02631768027502576\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191259-ucbas7xx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/ucbas7xx\" target=\"_blank\">morning-sweep-9</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea507538b00549cc80e73b7bbb26cbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">morning-sweep-9</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/ucbas7xx\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/ucbas7xx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191259-ucbas7xx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ucbas7xx errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ucbas7xx errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uuys500w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003706147610991958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 394\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.06451746584476475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191320-uuys500w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/uuys500w\" target=\"_blank\">cool-sweep-10</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0cdd25134648d590666e6ff721f8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cool-sweep-10</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/uuys500w\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/uuys500w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191320-uuys500w\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run uuys500w errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run uuys500w errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: guuq50rn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00026748756962351514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01280856358757562\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191341-guuq50rn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/guuq50rn\" target=\"_blank\">astral-sweep-11</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dc238c6af24d13a6ee1a4fde17ede3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">astral-sweep-11</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/guuq50rn\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/guuq50rn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191341-guuq50rn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run guuq50rn errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run guuq50rn errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p8e30l8f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002082346658616687\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 446\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.05773157064213042\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191403-p8e30l8f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/p8e30l8f\" target=\"_blank\">gallant-sweep-12</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174385a756d244a88de399b25d37c327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">gallant-sweep-12</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/p8e30l8f\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/p8e30l8f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191403-p8e30l8f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run p8e30l8f errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run p8e30l8f errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ivuha9r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.221382892536256e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.011038379743971377\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191423-5ivuha9r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/5ivuha9r\" target=\"_blank\">stilted-sweep-13</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b122245af5e54188bf849e86ff190c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stilted-sweep-13</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/5ivuha9r\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/5ivuha9r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191423-5ivuha9r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5ivuha9r errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5ivuha9r errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1c6hs6iy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003643041898070543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.038948635594846806\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191445-1c6hs6iy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/1c6hs6iy\" target=\"_blank\">gallant-sweep-14</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca25f8dfcfb64765b348d93f644b430c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">gallant-sweep-14</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/1c6hs6iy\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/1c6hs6iy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191445-1c6hs6iy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1c6hs6iy errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 1c6hs6iy errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wn7zae57 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00010257153071039308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.046312770525027705\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191505-wn7zae57</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/wn7zae57\" target=\"_blank\">swept-sweep-15</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ede3b7586a42d8ab3f637820df2a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">swept-sweep-15</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/wn7zae57\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/wn7zae57</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191505-wn7zae57\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run wn7zae57 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wn7zae57 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iflhvtq8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001267822734715245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.033640531566015305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191527-iflhvtq8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/iflhvtq8\" target=\"_blank\">true-sweep-16</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31726817c750432ca1417aae22fc2e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">true-sweep-16</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/iflhvtq8\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/iflhvtq8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191527-iflhvtq8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run iflhvtq8 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run iflhvtq8 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w71wdbhp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.867542368145006e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.027223375089084935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191548-w71wdbhp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/w71wdbhp\" target=\"_blank\">wise-sweep-17</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e463aa348c43aea2f5f9a54e7b3725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wise-sweep-17</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/w71wdbhp\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/w71wdbhp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191548-w71wdbhp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run w71wdbhp errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run w71wdbhp errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b2e1lt4y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000272957551224264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.07298471333191166\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191609-b2e1lt4y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/b2e1lt4y\" target=\"_blank\">lilac-sweep-18</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da95e9411be4bf4b5675090fab71069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lilac-sweep-18</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/b2e1lt4y\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/b2e1lt4y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191609-b2e1lt4y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run b2e1lt4y errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run b2e1lt4y errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 326wnd27 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003520495025801806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 195\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.08002409953734464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191630-326wnd27</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/326wnd27\" target=\"_blank\">morning-sweep-19</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f6432fc66044f5a9bd94f1f8477eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">morning-sweep-19</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/326wnd27\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/326wnd27</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191630-326wnd27\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 326wnd27 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 326wnd27 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q9xvbu0c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00036412557860978967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 434\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0306442807755681\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191651-q9xvbu0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/q9xvbu0c\" target=\"_blank\">different-sweep-20</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f45f744909474d912a911907998128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">different-sweep-20</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/q9xvbu0c\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/q9xvbu0c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191651-q9xvbu0c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run q9xvbu0c errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run q9xvbu0c errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1atdlkzi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00018389862989609196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.02010061018519804\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191719-1atdlkzi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/1atdlkzi\" target=\"_blank\">solar-sweep-21</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0140753d5898461aaa0c928084bb16ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-sweep-21</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/1atdlkzi\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/1atdlkzi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191719-1atdlkzi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1atdlkzi errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 1atdlkzi errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jry7wcgu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00012699238435361497\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.04565692632189629\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191736-jry7wcgu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/jry7wcgu\" target=\"_blank\">woven-sweep-22</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abedea1f49fb47b7a1379e3a3eeb76fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">woven-sweep-22</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/jry7wcgu\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/jry7wcgu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191736-jry7wcgu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run jry7wcgu errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jry7wcgu errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g9xi3hq4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003742402268976684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 498\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.07276651075862653\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191757-g9xi3hq4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/g9xi3hq4\" target=\"_blank\">scarlet-sweep-23</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dd9dd450dc41d8972900acf007a185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">scarlet-sweep-23</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/g9xi3hq4\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/g9xi3hq4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191757-g9xi3hq4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run g9xi3hq4 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run g9xi3hq4 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p1egywht with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001701477989449573\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 65\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.016798566864271357\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191819-p1egywht</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/p1egywht\" target=\"_blank\">colorful-sweep-24</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e532d97918428089a442cbb4e16384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">colorful-sweep-24</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/p1egywht\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/p1egywht</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191819-p1egywht\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run p1egywht errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run p1egywht errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2dfomikb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00039862207893306105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.02290791752562364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191839-2dfomikb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/2dfomikb\" target=\"_blank\">glowing-sweep-25</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a461fa78df064a6095c59fb779670bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glowing-sweep-25</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/2dfomikb\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/2dfomikb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191839-2dfomikb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2dfomikb errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2dfomikb errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r38ssmsb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003943321153477537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.09776875475262468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191901-r38ssmsb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/r38ssmsb\" target=\"_blank\">apricot-sweep-26</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91490d386aa4178927fe1d53b823595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">apricot-sweep-26</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/r38ssmsb\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/r38ssmsb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191901-r38ssmsb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run r38ssmsb errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run r38ssmsb errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g4s5hlyy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002795966019391381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.07207265617353181\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191921-g4s5hlyy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/g4s5hlyy\" target=\"_blank\">polar-sweep-27</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ee916016df4a9eadfb04f12c908363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polar-sweep-27</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/g4s5hlyy\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/g4s5hlyy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191921-g4s5hlyy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run g4s5hlyy errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run g4s5hlyy errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3drd392x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 7.299895402822348e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 61\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.058781957649614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_191943-3drd392x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/3drd392x\" target=\"_blank\">ancient-sweep-28</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7153b0289af54380b7d050c88150a8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ancient-sweep-28</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/3drd392x\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/3drd392x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_191943-3drd392x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 3drd392x errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 3drd392x errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yf10nz67 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002385671913305935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 477\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.03927415431877498\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192004-yf10nz67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/yf10nz67\" target=\"_blank\">electric-sweep-29</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bd21b1cc3e4347879b940613e8674b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">electric-sweep-29</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/yf10nz67\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/yf10nz67</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192004-yf10nz67\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run yf10nz67 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run yf10nz67 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nfe4bfb0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00013757260566800918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 450\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.07017498276943586\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192025-nfe4bfb0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/nfe4bfb0\" target=\"_blank\">lemon-sweep-30</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d15288a0194555a3be34a18f407883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lemon-sweep-30</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/nfe4bfb0\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/nfe4bfb0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192025-nfe4bfb0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run nfe4bfb0 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run nfe4bfb0 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gm76rbc7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00038998423499655267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.08329291648318497\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192046-gm76rbc7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/gm76rbc7\" target=\"_blank\">rural-sweep-31</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d0044077884f14bc41d0145fc6ab58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rural-sweep-31</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/gm76rbc7\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/gm76rbc7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192046-gm76rbc7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run gm76rbc7 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run gm76rbc7 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: et0kctzk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002654311202176218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.06486263500284412\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192128-et0kctzk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/et0kctzk\" target=\"_blank\">happy-sweep-32</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934db57b1d4d4be788ad3bbdfa4090e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">happy-sweep-32</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/et0kctzk\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/et0kctzk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192128-et0kctzk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run et0kctzk errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run et0kctzk errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kekgni7n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.547192529914793e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 483\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.08836456096624985\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192149-kekgni7n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/kekgni7n\" target=\"_blank\">polar-sweep-33</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6931371f46a45c49af274024f7d133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polar-sweep-33</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/kekgni7n\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/kekgni7n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192149-kekgni7n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run kekgni7n errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run kekgni7n errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d69jwsj7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002626612353357536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 407\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.09329356816053472\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192210-d69jwsj7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/d69jwsj7\" target=\"_blank\">kind-sweep-34</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a223cf7685714c85bf6ce421792c98ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">kind-sweep-34</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/d69jwsj7\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/d69jwsj7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192210-d69jwsj7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run d69jwsj7 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run d69jwsj7 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f4ncs6yh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5.88547175294001e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.025004347847352695\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192231-f4ncs6yh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/f4ncs6yh\" target=\"_blank\">silver-sweep-35</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ee492babff4674b94f2a69ae741249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">silver-sweep-35</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/f4ncs6yh\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/f4ncs6yh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192231-f4ncs6yh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run f4ncs6yh errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run f4ncs6yh errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j65ptl7c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00024829447983714244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0930451697530444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192253-j65ptl7c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/j65ptl7c\" target=\"_blank\">summer-sweep-36</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f329194900488693425e6dbcf8ac11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">summer-sweep-36</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/j65ptl7c\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/j65ptl7c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192253-j65ptl7c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run j65ptl7c errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run j65ptl7c errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3fn9mlau with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003493235399050259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 185\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.029303013069968006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192314-3fn9mlau</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/3fn9mlau\" target=\"_blank\">efficient-sweep-37</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2949dddc759f48b68854344942049422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">efficient-sweep-37</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/3fn9mlau\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/3fn9mlau</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192314-3fn9mlau\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 3fn9mlau errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 3fn9mlau errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 45pgdibb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00014807235095046516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 78\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.03369718113478768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192335-45pgdibb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/45pgdibb\" target=\"_blank\">breezy-sweep-38</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a9fbeb25d44adcae9899fc0052a796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">breezy-sweep-38</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/45pgdibb\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/45pgdibb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192335-45pgdibb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 45pgdibb errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 45pgdibb errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ti48dzoz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002432189051741772\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 450\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.09276573856440264\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192356-ti48dzoz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/ti48dzoz\" target=\"_blank\">lively-sweep-39</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3d281a43674980aa1db86f509392fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lively-sweep-39</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/ti48dzoz\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/ti48dzoz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192356-ti48dzoz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ti48dzoz errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ti48dzoz errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oc5u4y5v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00022833068362004404\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 391\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.08195133909261795\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192418-oc5u4y5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/oc5u4y5v\" target=\"_blank\">peachy-sweep-40</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb9429c1c9743fa8fb7cde7b97ea967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">peachy-sweep-40</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/oc5u4y5v\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/oc5u4y5v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192418-oc5u4y5v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run oc5u4y5v errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run oc5u4y5v errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5l5of88u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00038630373080544145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.04089447105660198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192438-5l5of88u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/5l5of88u\" target=\"_blank\">spring-sweep-41</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ef84801f6848e482f8a412aefca7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">spring-sweep-41</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/5l5of88u\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/5l5of88u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192438-5l5of88u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5l5of88u errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5l5of88u errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x6ewr100 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000397163715123622\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 341\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.07772144769977846\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192500-x6ewr100</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/x6ewr100\" target=\"_blank\">happy-sweep-42</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8525dbd3d5f54f5d8c4a58f0cd7d494a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">happy-sweep-42</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/x6ewr100\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/x6ewr100</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192500-x6ewr100\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run x6ewr100 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run x6ewr100 errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4dousrdn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5.67755274439378e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 310\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.04718409082624141\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\GCM\\Desktop\\SASDGHUB\\wandb\\run-20220818_192521-4dousrdn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/4dousrdn\" target=\"_blank\">youthful-sweep-43</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/mymmnvmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47786523a4e14478baa5235c5816e022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">youthful-sweep-43</strong>: <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/4dousrdn\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/4dousrdn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220818_192521-4dousrdn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 4dousrdn errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4dousrdn errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 12.00 GiB total capacity; 10.77 GiB already allocated; 0 bytes free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n"
     ]
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args = MultiLabelClassificationArgs(fp16= False,\n",
    "                                          threshold=0.8,\n",
    "                                          manual_seed = 4,\n",
    "                                          use_multiprocessing = True,\n",
    "                                          overwrite_output_dir=True,\n",
    "                                          evaluate_during_training = True,\n",
    "                                          # wandb_project = 'sasdghub_ml_classify',\n",
    "                                          # wandb_kwargs={\n",
    "                                          #     'entity':'sasdghub'\n",
    "                                          #              },\n",
    "                                          # num_train_epochs=1,\n",
    "                                          # train_batch_size= 16,\n",
    "                                          # eval_batch_size= 64,\n",
    "                                          # warmup_steps= 500,\n",
    "                                          # weight_decay= 0.01,\n",
    "                                          # logging_steps= 10,\n",
    "                                          # learning_rate= 5e-5\n",
    "                                          \n",
    "                                         )\n",
    "\n",
    "def train():\n",
    "    \n",
    "    # Initialize a new wandb run \n",
    "    wandb.init()\n",
    "\n",
    "    # Create a MultiLabelClassificationModel\n",
    "    model = MultiLabelClassificationModel(\n",
    "        \"xlnet\",\n",
    "        \"xlnet-base-cased\",\n",
    "        num_labels=label_count,\n",
    "        args=model_args,\n",
    "        use_cuda=cuda_available,\n",
    "        pos_weight=list(1-class_weight),\n",
    "        # show_running_loss=True,\n",
    "        sweep_config=wandb.config,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.train_model(train_df,\n",
    "                      verbose=True,\n",
    "                      eval_df=eval_df)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    result, model_outputs, wrong_predictions = model.eval_model(\n",
    "        eval_df,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Sync wandb\n",
    "    wandb.join()\n",
    "    \n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583face1-70f2-44ad-a913-1d66137b0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with optimal parameters\n",
    "# # logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# # Optional model configuration\n",
    "# model_args = MultiLabelClassificationArgs(fp16= False,\n",
    "#                                           threshold=0.8,\n",
    "#                                           manual_seed = 4,\n",
    "#                                           use_multiprocessing = True,\n",
    "#                                           overwrite_output_dir=True,\n",
    "#                                           evaluate_during_training = True,\n",
    "#                                           wandb_project = 'sasdghub_ml_classify',\n",
    "#                                           wandb_kwargs={\n",
    "#                                               'entity':'sasdghub'\n",
    "#                                                        },\n",
    "#                                           num_train_epochs=1,\n",
    "#                                           train_batch_size= 16,\n",
    "#                                           eval_batch_size= 64,\n",
    "#                                           warmup_steps= 500,\n",
    "#                                           weight_decay= 0.01,\n",
    "#                                           logging_steps= 10,\n",
    "#                                           learning_rate= 5e-5\n",
    "                                          \n",
    "#                                          )\n",
    "\n",
    "# # Create a MultiLabelClassificationModel\n",
    "# model = MultiLabelClassificationModel(\n",
    "#     \"xlnet\",\n",
    "#     \"xlnet-base-cased\",\n",
    "#     num_labels=label_count,\n",
    "#     args=model_args,\n",
    "#     use_cuda=cuda_available,\n",
    "#     pos_weight=list(1-class_weight),\n",
    "    # show_running_loss=True,\n",
    "    # sweep_config=wandb.config,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# model.train_model(train_df,\n",
    "#                   verbose=True,\n",
    "#                   eval_df=eval_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# result, model_outputs, wrong_predictions = model.eval_model(\n",
    "#     eval_df,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d8cf8-6bfa-4d5e-9d95-08f977d822ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions with the model\n",
    "# predictions, raw_outputs = model.predict([\"sanitation and clean drinking water\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee789c-39c2-4883-b216-35734f534082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# modify so it can digest longer text (split training data into paragraphs that can be digested)\n",
    "\n",
    "# save model\n",
    "# apply new model in apply script\n",
    "\n",
    "# make testing script with ROC curves and confusion matrices\n",
    "\n",
    "# get model embedding\n",
    "# train classifier for targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
