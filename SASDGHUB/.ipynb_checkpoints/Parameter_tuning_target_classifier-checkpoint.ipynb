{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fd34e1-4700-417a-b95f-42fd97e908b1",
   "metadata": {},
   "source": [
    "### Target classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689e0297-9b06-429a-aeeb-7741fcefbc9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T16:07:30.277799Z",
     "iopub.status.busy": "2022-09-01T16:07:30.277799Z",
     "iopub.status.idle": "2022-09-01T16:07:34.718833Z",
     "shell.execute_reply": "2022-09-01T16:07:34.717826Z",
     "shell.execute_reply.started": "2022-09-01T16:07:30.277799Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import wandb\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7886bd55-872c-483c-a0c4-42ae8765d49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T16:07:34.719826Z",
     "iopub.status.busy": "2022-09-01T16:07:34.719826Z",
     "iopub.status.idle": "2022-09-01T16:07:35.467854Z",
     "shell.execute_reply": "2022-09-01T16:07:35.466855Z",
     "shell.execute_reply.started": "2022-09-01T16:07:34.719826Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('OneHot_Combined_cln_utf8.tsv', sep='\\t')\n",
    "data = data[data['source']!='SASDG_Hub'] #keep the articles classified by Willem separate as an unseen testing set\n",
    "# data = data.iloc[-1000:,:] # select a small subset of the data (last 1000 rows)\n",
    "\n",
    "# import target data\n",
    "target_df = pd.read_csv('Targets.csv', sep=';')\n",
    "targets_lst = target_df['target'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1160642-d5b4-4ed4-9461-a49a8557e658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T16:07:35.468829Z",
     "iopub.status.busy": "2022-09-01T16:07:35.468829Z",
     "iopub.status.idle": "2022-09-01T16:07:35.483837Z",
     "shell.execute_reply": "2022-09-01T16:07:35.482842Z",
     "shell.execute_reply.started": "2022-09-01T16:07:35.468829Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to apply model to text\n",
    "def classify_sdg(text_lst):\n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    # import model from path (this path is the directory with all the model files)\n",
    "    sdg_model = MultiLabelClassificationModel(\n",
    "            \"xlnet\",\n",
    "            \"outputs/best_model/\",   #os.getcwd()+\"\\\\outputs\\\\best_model\\\\\", #C:\\Users\\GCM\\Desktop\\GIT_REPOS\\Transformers_simple_wandb_experiments\\SASDGHUB\\\n",
    "            num_labels=17,\n",
    "            use_cuda=cuda_available,\n",
    "            )\n",
    "    predictions, raw_outputs = sdg_model.predict(text_lst)\n",
    "    return(predictions, raw_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e9d699-4603-46fa-8e6f-1598361379e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T16:07:35.485834Z",
     "iopub.status.busy": "2022-09-01T16:07:35.485834Z",
     "iopub.status.idle": "2022-09-01T16:07:35.513872Z",
     "shell.execute_reply": "2022-09-01T16:07:35.512891Z",
     "shell.execute_reply.started": "2022-09-01T16:07:35.485834Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to classify targets\n",
    "def classify_sdg_target(text_lst, \n",
    "                        target_data_path='Targets.csv',\n",
    "                        run_isomap=True, # run faster for multiple samples otherwise (target_data_path required when this is False) (do not use when only one sample)\n",
    "                        target_embedding_reduced_path=None, # load a previously calculated and reduced embedding for the targets 'outputs/targets_embedded_reduced_gpt2_2D.csv'\n",
    "                        isomap_dims = 2,\n",
    "                        isomap_neigbors = 5, # has to be <= len(text_lst) a.k.a n_samples\n",
    "                        pre_trained_model_type='gpt2', \n",
    "                        pre_trained_model_name='gpt2',\n",
    "                        target_threshold_val=0.5):\n",
    "    \n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "\n",
    "    # define and load model from hugging face\n",
    "    model_args = ModelArgs(max_seq_length=1024)\n",
    "    # model import\n",
    "    model = RepresentationModel(\n",
    "        pre_trained_model_type,\n",
    "        pre_trained_model_name, #gpt2 , gpt2-large\n",
    "        args=model_args,\n",
    "    )\n",
    "    \n",
    "    # classify sdg of text\n",
    "    sdg_predictions, sdg_raw_outputs = classify_sdg(text_lst)\n",
    "    # get embeddings of text\n",
    "    word_embeddings = model.encode_sentences(text_lst, combine_strategy=\"mean\")\n",
    "    \n",
    "    \n",
    "    # ISOMAP\n",
    "    if run_isomap==True: \n",
    "        # reduce isomap_neigbors to fit the number of samples\n",
    "        n_samples = len(text_lst)\n",
    "        if n_samples < isomap_neigbors:\n",
    "            isomap_neigbors = np.max([1,n_samples-1])\n",
    "            print('Reduced isomap_n_neigbors to: ', isomap_neigbors)\n",
    "\n",
    "        # reduce dimensions of embeddings to 2 (can be reduced to higher dimensions)\n",
    "        isomap = Isomap(n_components=isomap_dims, n_neighbors=isomap_neigbors-1) # input is an array with samples x features\n",
    "        word_embeddings_transformed = isomap.fit_transform(word_embeddings)\n",
    "\n",
    "        if target_embedding_reduced_path == None:\n",
    "            # load pre-calculated embeddings\n",
    "            target_df = pd.read_csv(target_data_path, sep=';')\n",
    "            # get sentence list from target data\n",
    "            target_sentence_list = target_df['text'].tolist()\n",
    "            # get embeddings of targets\n",
    "            target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "            target_embeddings_transformed = isomap.fit_transform(target_embeddings)\n",
    "\n",
    "            # add labels to reduced embeddings\n",
    "            target_trans_df = pd.DataFrame(target_embeddings_transformed)\n",
    "            target_trans_df['target'] = target_df['target']\n",
    "            target_trans_df['sdg'] = target_df['sdg']\n",
    "            target_trans_df.to_csv('outputs/targets_embedded_reduced_'+pre_trained_model_name+'_'+str(isomap_dims)+'D.csv', index=False)\n",
    "\n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "            \n",
    "        else:\n",
    "            # import reduced embedding of targets\n",
    "            target_trans_df = pd.read_csv(target_embedding_reduced_path, sep=',')\n",
    "            \n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "\n",
    "        # plot embeddings if they are 2D\n",
    "        if isomap_dims ==2:\n",
    "            trans_df = pd.DataFrame(word_embeddings_transformed)\n",
    "            trans_df['target'] = target_df['target']\n",
    "            trans_df['sdg'] = target_df['sdg']\n",
    "            trans_df.plot.scatter(0,1,c='sdg', colormap='viridis') # colour by sdg\n",
    "            plt.title('Isomap 2D plot of text embedding')\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        # load pre-calculated embeddings\n",
    "        target_df = pd.read_csv(target_data_path, sep=';')\n",
    "        # get sentence list from target data\n",
    "        target_sentence_list = target_df['text'].tolist()\n",
    "        # get embeddings of targets\n",
    "        target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "        \n",
    "        # define source and target for KNN\n",
    "        Y = pd.DataFrame(target_embeddings)\n",
    "        Y['target'] = target_df['target']\n",
    "        Y['sdg'] = target_df['sdg']\n",
    "        X = pd.DataFrame(word_embeddings)\n",
    "        idx_ar = np.array(range(0,17), np.int64)\n",
    "        sdg_label_lst = []\n",
    "        for row in (np.array(sdg_predictions)==1):\n",
    "            sdg_label_lst.append(idx_ar[row])\n",
    "        X['sdg'] = sdg_label_lst\n",
    "        sdg_prob_lst = []\n",
    "        for row in sdg_raw_outputs:\n",
    "            sdg_prob_lst.append(row)\n",
    "        X['sdg_probability'] = sdg_prob_lst\n",
    "    \n",
    "    # use cosine similarity Kmeans variant to classify targets\n",
    "    # define final results table\n",
    "    results_df = pd.DataFrame()\n",
    "    results_df['text'] = text_lst\n",
    "    results_df['sdg'] = X['sdg']\n",
    "    results_df['sdg_probability'] = X['sdg_probability']\n",
    "    # calculate pairwise cosine similarity between targets and text list\n",
    "    similarity_ar = cosine_similarity(X.loc[:, ~X.columns.isin(['sdg', 'sdg_probability'])], \n",
    "                                  Y.loc[:, ~Y.columns.isin(['sdg', 'target'])],\n",
    "                                 )\n",
    "    results_df['target_similarity'] = similarity_ar.tolist()\n",
    "    # select targets on distance sdg and threshold\n",
    "    targets_ar = np.array(Y['target'])\n",
    "    targets_full_ar = np.tile(targets_ar, (len(text_lst), 1)) \n",
    "    sdg_ar = np.array(Y['sdg'])\n",
    "    sdg_full_ar = np.tile(sdg_ar, (len(text_lst), 1))\n",
    "    sdg_select_ar = np.array(X['sdg'])\n",
    "    \n",
    "    # select classified SDGs\n",
    "    sdg_onehot_lst = []\n",
    "    for i in range(len(sdg_full_ar)):\n",
    "        isin_ar = np.isin(sdg_full_ar[i], sdg_select_ar[i])\n",
    "        sdg_onehot_lst.append(isin_ar)\n",
    "    sdg_onehot_ar = np.vstack(sdg_onehot_lst)\n",
    "    target_onehot_ar = (similarity_ar>=target_threshold_val)*sdg_onehot_ar\n",
    "    target_label_lst = (targets_full_ar*target_onehot_ar).tolist()\n",
    "    results_df['target'] = [[ele for ele in sub if ele != ''] for sub in target_label_lst]\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a510ed7c-994a-4ae9-9793-a90d6c452f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T16:07:35.515871Z",
     "iopub.status.busy": "2022-09-01T16:07:35.514872Z",
     "iopub.status.idle": "2022-09-01T16:07:35.528865Z",
     "shell.execute_reply": "2022-09-01T16:07:35.528865Z",
     "shell.execute_reply.started": "2022-09-01T16:07:35.515871Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "# create functions for additional evaluation outputs\n",
    "def acc_result(true, pred, targets_lst=targets_lst):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    acc_sum = 0\n",
    "    for i in range(true.shape[0]):\n",
    "        acc_sum += sklearn.metrics.accuracy_score(true[i], pred[i])    \n",
    "    acc = acc_sum/true.shape[0]\n",
    "    print(np.unique(true))\n",
    "    print(np.unique(pred))\n",
    "    return acc\n",
    "\n",
    "def f1_macro_result(true, pred, targets_lst=targets_lst):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    f1 = sklearn.metrics.f1_score(true, pred, average='samples')\n",
    "    return f1\n",
    "\n",
    "def cm_wandb_result(true, pred, targets_lst=targets_lst):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    # modify labels and fill all combinations to use wand multiclass confusion matrix visually\n",
    "    d=true-pred\n",
    "    t_d = (d==1)\n",
    "    p_d = (d==-1)\n",
    "    n_d = (d==0)\n",
    "    idx_ar = np.array(range(0,true.shape[1]))\n",
    "    idx = np.tile(idx_ar,true.shape[0]).reshape(true.shape[0],true.shape[1])\n",
    "    n_labels = idx[n_d]\n",
    "    t_lst = []\n",
    "    for row in t_d:\n",
    "        if row.sum()==0:\n",
    "            t_lst.append(idx_ar)\n",
    "        else:\n",
    "            t_lst.append(idx_ar[row])\n",
    "    p_lst = []\n",
    "    for row in p_d:\n",
    "        if row.sum()==0:\n",
    "            p_lst.append(idx_ar)\n",
    "        else:\n",
    "            p_lst.append(idx_ar[row])\n",
    "    for i in range(len(t_lst)):\n",
    "        fill_ar = np.array(list(itertools.product(p_lst[i], t_lst[i])))\n",
    "        t_labels = np.append(n_labels,fill_ar[:,0].tolist())\n",
    "        p_labels = np.append(n_labels,fill_ar[:,1].tolist())\n",
    "    wandb_cm = wandb.plot.confusion_matrix(probs=None, y_true=t_labels, preds=p_labels, class_names=sdg_lst)\n",
    "    return wandb_cm\n",
    "\n",
    "def cm_result(true, pred, targets_lst=targets_lst):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    cm = sklearn.metrics.multilabel_confusion_matrix(true, pred)\n",
    "    return cm\n",
    "\n",
    "def cm_avg_result(true, pred, targets_lst=targets_lst):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    cm = sklearn.metrics.multilabel_confusion_matrix(true, pred)\n",
    "    cm_avg = cm.sum(axis=0)/true.shape[1]\n",
    "    return cm_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d533a0-2798-47f2-82d9-7b84eb7b72cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T16:07:35.530864Z",
     "iopub.status.busy": "2022-09-01T16:07:35.529869Z",
     "iopub.status.idle": "2022-09-01T16:07:38.249206Z",
     "shell.execute_reply": "2022-09-01T16:07:38.248207Z",
     "shell.execute_reply.started": "2022-09-01T16:07:35.530864Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 1dfa3lpo\n",
      "Sweep URL: https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/1dfa3lpo\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # bayes, grid, random\n",
    "    \"metric\": {\"name\": \"f1_macro\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"run_isomap\": {\"values\": [True, False]},\n",
    "        \"isomap_dims\": {\"min\": 2, \"max\": 500},\n",
    "        \"isomap_neigbors\":{\"min\": 2, \"max\": 100},\n",
    "        \"target_threshold_val\":{\"min\": 0.0, \"max\": 1.0},\n",
    "    },\n",
    "}\n",
    "\n",
    "# define the project and entity under which the outputs will be recorded in wandb\n",
    "sweep_id = wandb.sweep(sweep_config, entity='sasdghub', project=\"sasdghub_ml_classify\")\n",
    "\n",
    "# Set logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa68605-b9ff-4e82-952e-4922952acf13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T16:07:38.251218Z",
     "iopub.status.busy": "2022-09-01T16:07:38.251218Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.agents.pyagent:Starting sweep agent: entity=None, project=None, count=None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tud98ku7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tisomap_dims: 394\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tisomap_neigbors: 47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trun_isomap: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_threshold_val: 0.32788442175334553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m (\u001b[33msasdghub\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\gcmar\\Desktop\\GIT_REPOS\\Transformers_simple_wandb_experiments\\SASDGHUB\\wandb\\run-20220901_120741-tud98ku7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/runs/tud98ku7\" target=\"_blank\">driven-sweep-1</a></strong> to <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/1dfa3lpo\" target=\"_blank\">https://wandb.ai/sasdghub/sasdghub_ml_classify/sweeps/1dfa3lpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2ForTextRepresentation: ['h.4.ln_1.bias', 'h.8.mlp.c_fc.weight', 'h.5.mlp.c_fc.weight', 'h.4.ln_2.weight', 'h.0.attn.bias', 'h.6.ln_1.bias', 'h.6.attn.c_attn.bias', 'h.9.attn.c_attn.weight', 'ln_f.weight', 'h.8.ln_2.bias', 'h.8.ln_1.bias', 'h.9.ln_2.weight', 'h.1.mlp.c_proj.bias', 'h.11.ln_1.bias', 'h.3.attn.c_attn.weight', 'h.0.mlp.c_proj.bias', 'h.2.attn.bias', 'h.5.mlp.c_proj.weight', 'h.5.ln_2.weight', 'h.7.attn.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.8.mlp.c_proj.bias', 'h.11.attn.c_attn.bias', 'h.0.attn.c_attn.bias', 'h.4.mlp.c_fc.bias', 'h.11.attn.c_attn.weight', 'h.0.ln_1.weight', 'h.1.mlp.c_fc.weight', 'h.2.ln_1.bias', 'h.11.mlp.c_proj.bias', 'h.3.mlp.c_proj.weight', 'h.8.attn.c_proj.bias', 'h.9.mlp.c_proj.bias', 'h.7.attn.c_attn.bias', 'wpe.weight', 'h.7.mlp.c_fc.bias', 'h.10.attn.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.6.ln_2.bias', 'h.6.mlp.c_proj.bias', 'h.0.attn.c_proj.bias', 'h.9.attn.c_attn.bias', 'h.7.attn.c_proj.bias', 'h.4.ln_2.bias', 'h.8.mlp.c_fc.bias', 'h.2.mlp.c_proj.bias', 'h.5.attn.bias', 'h.1.ln_1.bias', 'ln_f.bias', 'h.1.attn.c_attn.weight', 'h.5.ln_1.weight', 'h.6.mlp.c_proj.weight', 'h.1.attn.c_proj.weight', 'h.9.ln_2.bias', 'h.11.attn.c_proj.weight', 'h.1.ln_2.bias', 'h.2.ln_2.weight', 'h.0.ln_2.weight', 'h.6.attn.bias', 'h.5.attn.c_attn.bias', 'h.8.attn.c_attn.weight', 'h.11.ln_2.bias', 'h.11.mlp.c_fc.weight', 'h.8.attn.bias', 'h.9.ln_1.bias', 'h.9.attn.c_proj.weight', 'h.0.mlp.c_fc.weight', 'h.2.mlp.c_proj.weight', 'h.3.attn.c_proj.bias', 'h.8.ln_1.weight', 'h.10.ln_2.bias', 'h.7.attn.bias', 'h.9.mlp.c_fc.bias', 'h.1.mlp.c_proj.weight', 'h.0.attn.c_proj.weight', 'h.5.ln_1.bias', 'h.8.attn.c_attn.bias', 'h.3.attn.c_proj.weight', 'h.9.attn.bias', 'h.10.attn.c_attn.weight', 'h.11.ln_1.weight', 'h.10.attn.c_proj.bias', 'h.10.mlp.c_proj.weight', 'h.7.attn.c_attn.weight', 'h.5.mlp.c_proj.bias', 'h.11.attn.c_proj.bias', 'h.11.mlp.c_proj.weight', 'h.4.mlp.c_fc.weight', 'h.4.ln_1.weight', 'h.10.mlp.c_fc.weight', 'h.4.attn.c_proj.bias', 'h.3.ln_2.weight', 'h.3.ln_1.bias', 'h.0.ln_2.bias', 'h.3.attn.bias', 'h.6.attn.c_attn.weight', 'h.6.mlp.c_fc.bias', 'h.7.mlp.c_proj.weight', 'h.9.mlp.c_fc.weight', 'h.2.mlp.c_fc.bias', 'h.7.ln_2.bias', 'h.10.attn.c_attn.bias', 'h.1.attn.bias', 'h.3.mlp.c_fc.bias', 'h.6.attn.c_proj.bias', 'h.1.ln_1.weight', 'h.1.attn.c_proj.bias', 'h.3.ln_1.weight', 'h.4.mlp.c_proj.bias', 'h.8.attn.c_proj.weight', 'h.6.ln_1.weight', 'h.10.mlp.c_fc.bias', 'h.4.attn.c_attn.bias', 'h.7.ln_2.weight', 'h.9.ln_1.weight', 'h.2.mlp.c_fc.weight', 'h.7.ln_1.bias', 'h.5.attn.c_attn.weight', 'h.9.mlp.c_proj.weight', 'h.0.attn.c_attn.weight', 'h.10.ln_1.weight', 'h.1.ln_2.weight', 'h.8.ln_2.weight', 'h.0.mlp.c_proj.weight', 'h.5.attn.c_proj.bias', 'h.11.mlp.c_fc.bias', 'h.5.mlp.c_fc.bias', 'h.7.ln_1.weight', 'h.2.ln_2.bias', 'h.2.attn.c_attn.bias', 'h.4.attn.c_attn.weight', 'h.0.ln_1.bias', 'wte.weight', 'h.4.attn.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.3.mlp.c_proj.bias', 'h.6.ln_2.weight', 'h.11.ln_2.weight', 'h.2.attn.c_proj.weight', 'h.0.mlp.c_fc.bias', 'h.4.attn.bias', 'h.8.mlp.c_proj.weight', 'h.4.mlp.c_proj.weight', 'h.2.ln_1.weight', 'h.11.attn.bias', 'h.6.mlp.c_fc.weight', 'h.3.mlp.c_fc.weight', 'h.10.ln_2.weight', 'h.3.ln_2.bias', 'h.1.mlp.c_fc.bias', 'h.7.mlp.c_proj.bias', 'h.9.attn.c_proj.bias', 'h.10.ln_1.bias', 'h.2.attn.c_attn.weight', 'h.10.attn.bias', 'h.7.mlp.c_fc.weight', 'h.3.attn.c_attn.bias', 'h.5.ln_2.bias', 'h.6.attn.c_proj.weight', 'h.5.attn.c_proj.weight']\n",
      "- This IS expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at gpt2 and are newly initialized: ['gpt2.h.6.attn.c_proj.bias', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.8.ln_1.bias', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.9.ln_1.weight', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.2.attn.masked_bias', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.10.ln_2.bias', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.2.attn.bias', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.4.attn.bias', 'gpt2.h.11.ln_1.weight', 'gpt2.h.5.attn.masked_bias', 'gpt2.h.11.attn.masked_bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.4.attn.masked_bias', 'gpt2.h.10.ln_1.bias', 'gpt2.h.0.ln_2.bias', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.8.ln_2.weight', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.6.ln_1.bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.1.attn.bias', 'gpt2.h.9.ln_1.bias', 'gpt2.h.10.attn.bias', 'gpt2.h.0.attn.bias', 'gpt2.h.1.ln_1.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.10.attn.masked_bias', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.9.ln_2.bias', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.5.attn.bias', 'gpt2.h.0.attn.masked_bias', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.h.7.attn.masked_bias', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.10.ln_2.weight', 'gpt2.h.6.attn.bias', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.10.ln_1.weight', 'gpt2.h.11.ln_2.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.7.ln_1.bias', 'gpt2.wte.weight', 'gpt2.h.3.ln_2.bias', 'gpt2.h.6.attn.masked_bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.7.ln_1.weight', 'gpt2.h.8.attn.bias', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.3.ln_2.weight', 'gpt2.h.2.ln_2.weight', 'gpt2.h.4.ln_2.weight', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.9.attn.bias', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.8.ln_2.bias', 'gpt2.h.4.ln_2.bias', 'gpt2.h.8.attn.masked_bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.7.ln_2.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.3.attn.masked_bias', 'gpt2.h.1.ln_2.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.9.attn.masked_bias', 'gpt2.h.7.attn.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.h.1.ln_2.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.2.ln_1.bias', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.6.ln_2.weight', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.5.ln_2.weight', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.4.ln_1.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.4.ln_1.bias', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.2.ln_1.weight', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.3.attn.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.11.ln_1.bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.ln_f.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.9.ln_2.weight', 'gpt2.h.1.attn.masked_bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.wpe.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.8.ln_1.weight', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.6.ln_1.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.5.ln_2.bias', 'gpt2.h.11.attn.bias', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.0.ln_1.bias', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.2.ln_2.bias', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.ln_f.weight', 'gpt2.h.1.ln_1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d073d23b7bf448298ba581980610074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ef9bb4e21145abb97b0a3951c6e548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use all the data that have targets to estimate accuracy of the \n",
    "val_data = data[data['target'].notna()][['abstract', 'target']].iloc[-1000:,:].reset_index(drop=True)\n",
    "val_text_lst = val_data['abstract'].tolist()\n",
    "\n",
    "# define the training function\n",
    "def train():\n",
    "    \n",
    "    # Initialize a new wandb run \n",
    "    wandb.init()\n",
    "        \n",
    "    results_df = classify_sdg_target(text_lst=val_text_lst,\n",
    "                                     target_data_path='Targets.csv',\n",
    "                                     run_isomap=wandb.config.run_isomap,\n",
    "                                     target_embedding_reduced_path=None,\n",
    "                                     isomap_dims=wandb.config.isomap_dims,\n",
    "                                     isomap_neigbors=wandb.config.isomap_neigbors,\n",
    "                                     pre_trained_model_type='gpt2', \n",
    "                                     pre_trained_model_name='gpt2',\n",
    "                                     target_threshold_val=wandb.config.target_threshold_val\n",
    "                                    )\n",
    "\n",
    "    acc = acc_result(true=val_data['target'], pred=results_df['target'])\n",
    "    f1 = f1_macro_result(true=val_data['target'], pred=results_df['target'])\n",
    "    cm = cm_result(true=val_data['target'], pred=results_df['target'])\n",
    "    cm_avg = cm_avg_result(true=val_data['target'], pred=results_df['target'])\n",
    "    cm_wandb = cm_wandb_result(true=val_data['target'], pred=results_df['target'])\n",
    "    \n",
    "    wandb.log({\"accuracy\": acc, \n",
    "               \"f1-macro\": f1,\n",
    "               \"confusion_matrix\": cm,\n",
    "               \"confusion_matrix_average\": cm_avg,\n",
    "               \"confusion_matrix_wandb\": cm_wandb,\n",
    "               \"run_isomap\":wandb.config.run_isomap,\n",
    "               \"isomap_dims\":wandb.config.isomap_dims,\n",
    "               \"isomap_neigbors\":wandb.config.isomap_neigbors,\n",
    "               \"target_threshold_val\":wandb.config.target_threshold_val\n",
    "              })\n",
    "    \n",
    "    # Sync wandb\n",
    "    wandb.join()\n",
    "\n",
    "# run the sweep and record results in wandb    \n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6834437-148d-49b5-8948-8fb1718de944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use all the data that have targets to estimate accuracy of the \n",
    "test_data = data[data['target'].notna()][['abstract', 'target']].iloc[-1000:,:].reset_index(drop=True)\n",
    "test_text_lst = test_data['abstract'].tolist()\n",
    "\n",
    "results_df = classify_sdg_target(text_lst=test_text_lst,\n",
    "                                 target_data_path='Targets.csv',\n",
    "                                 run_isomap=True, # run faster for multiple samples otherwise (target_data_path required when this is False) (do not use when only one sample)\n",
    "                                 target_embedding_reduced_path=None, # load a previously calculated and reduced embedding for the targets 'outputs/targets_embedded_reduced_gpt2_2D.csv'\n",
    "                                 isomap_dims = 2,\n",
    "                                 isomap_neigbors = 5, # has to be <= len(text_lst) a.k.a n_samples\n",
    "                                 pre_trained_model_type='gpt2', \n",
    "                                 pre_trained_model_name='gpt2',\n",
    "                                 target_threshold_val=0.5\n",
    "                                )\n",
    "\n",
    "acc = acc_result(true=test_data['target'], pred=results_df['target'])\n",
    "f1 = f1_macro_result(true=test_data['target'], pred=results_df['target'])\n",
    "cm = cm_result(true=test_data['target'], pred=results_df['target'])\n",
    "cm_avg = cm_avg_result(true=test_data['target'], pred=results_df['target'])\n",
    "\n",
    "print(acc, f1, cm_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02052a80-7128-47de-8902-06f3d9406a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create sweep for parameter tuning with wandb with evaluation and training fucntions\n",
    "# modify function to be used for testing both sdg and target classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
