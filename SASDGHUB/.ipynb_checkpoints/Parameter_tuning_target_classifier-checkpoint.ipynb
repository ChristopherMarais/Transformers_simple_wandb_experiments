{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fd34e1-4700-417a-b95f-42fd97e908b1",
   "metadata": {},
   "source": [
    "### Target classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689e0297-9b06-429a-aeeb-7741fcefbc9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T19:24:18.272704Z",
     "iopub.status.busy": "2022-08-30T19:24:18.272704Z",
     "iopub.status.idle": "2022-08-30T19:24:25.002077Z",
     "shell.execute_reply": "2022-08-30T19:24:25.001075Z",
     "shell.execute_reply.started": "2022-08-30T19:24:18.272704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7886bd55-872c-483c-a0c4-42ae8765d49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T19:24:25.004576Z",
     "iopub.status.busy": "2022-08-30T19:24:25.004576Z",
     "iopub.status.idle": "2022-08-30T19:24:25.839079Z",
     "shell.execute_reply": "2022-08-30T19:24:25.838080Z",
     "shell.execute_reply.started": "2022-08-30T19:24:25.004576Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('OneHot_Combined_cln_utf8.tsv', sep='\\t')\n",
    "data = data[data['source']!='SASDG_Hub'] #keep the articles classified by Willem separate as an unseen testing set\n",
    "data = data.iloc[-1000:,:] # select a small subset of the data (last 1000 rows)\n",
    "\n",
    "# import target data\n",
    "target_df = pd.read_csv('Targets.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1160642-d5b4-4ed4-9461-a49a8557e658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T19:24:25.841077Z",
     "iopub.status.busy": "2022-08-30T19:24:25.841077Z",
     "iopub.status.idle": "2022-08-30T19:24:25.855076Z",
     "shell.execute_reply": "2022-08-30T19:24:25.853575Z",
     "shell.execute_reply.started": "2022-08-30T19:24:25.841077Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to apply model to text\n",
    "def classify_sdg(text_lst):\n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    # import model from path (this path is the directory with all the model files)\n",
    "    sdg_model = MultiLabelClassificationModel(\n",
    "            \"xlnet\",\n",
    "            r\"C:\\Users\\gcmar\\Desktop\\GIT_REPOS\\Transformers_simple_wandb_experiments\\SASDGHUB\\outputs\\best_model\\\\\",\n",
    "            num_labels=17,\n",
    "            use_cuda=cuda_available,\n",
    "            )\n",
    "    predictions, raw_outputs = sdg_model.predict(text_lst)\n",
    "    return(predictions, raw_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e9d699-4603-46fa-8e6f-1598361379e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T19:24:25.857580Z",
     "iopub.status.busy": "2022-08-30T19:24:25.857076Z",
     "iopub.status.idle": "2022-08-30T19:24:26.254930Z",
     "shell.execute_reply": "2022-08-30T19:24:26.253928Z",
     "shell.execute_reply.started": "2022-08-30T19:24:25.857580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to classify targets\n",
    "def classify_sdg_target(text_lst, \n",
    "                        target_data_path='Targets.csv',\n",
    "                        run_isomap=True, # run faster for multiple samples otherwise (target_data_path required when this is False) (do not use when only one sample)\n",
    "                        target_embedding_reduced_path=None, # load a previously calculated and reduced embedding for the targets 'outputs/targets_embedded_reduced_gpt2_2D.csv'\n",
    "                        isomap_dims = 2,\n",
    "                        isomap_neigbors = 5, # has to be <= len(text_lst) a.k.a n_samples\n",
    "                        pre_trained_model_type='gpt2', \n",
    "                        pre_trained_model_name='gpt2',\n",
    "                        target_threshold_val=0.5):\n",
    "    \n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "\n",
    "    # define and load model from hugging face\n",
    "    model_args = ModelArgs(max_seq_length=1024)\n",
    "    # model import\n",
    "    model = RepresentationModel(\n",
    "        pre_trained_model_type,\n",
    "        pre_trained_model_name, #gpt2 , gpt2-large\n",
    "        args=model_args,\n",
    "    )\n",
    "    \n",
    "    # classify sdg of text\n",
    "    sdg_predictions, sdg_raw_outputs = classify_sdg(text_lst)\n",
    "    # get embeddings of text\n",
    "    word_embeddings = model.encode_sentences(text_lst, combine_strategy=\"mean\")\n",
    "    \n",
    "    \n",
    "    # ISOMAP\n",
    "    if run_isomap==True: \n",
    "        # reduce isomap_neigbors to fit the number of samples\n",
    "        n_samples = len(text_lst)\n",
    "        if n_samples < isomap_neigbors:\n",
    "            isomap_neigbors = np.max([1,n_samples-1])\n",
    "            print('Reduced isomap_n_neigbors to: ', isomap_neigbors)\n",
    "\n",
    "        # reduce dimensions of embeddings to 2 (can be reduced to higher dimensions)\n",
    "        isomap = Isomap(n_components=isomap_dims, n_neighbors=isomap_neigbors-1) # input is an array with samples x features\n",
    "        word_embeddings_transformed = isomap.fit_transform(word_embeddings)\n",
    "\n",
    "        if target_embedding_reduced_path == None:\n",
    "            # load pre-calculated embeddings\n",
    "            target_df = pd.read_csv(target_data_path, sep=';')\n",
    "            # get sentence list from target data\n",
    "            target_sentence_list = target_df['text'].tolist()\n",
    "            # get embeddings of targets\n",
    "            target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "            target_embeddings_transformed = isomap.fit_transform(target_embeddings)\n",
    "\n",
    "            # add labels to reduced embeddings\n",
    "            target_trans_df = pd.DataFrame(target_embeddings_transformed)\n",
    "            target_trans_df['target'] = target_df['target']\n",
    "            target_trans_df['sdg'] = target_df['sdg']\n",
    "            target_trans_df.to_csv('outputs/targets_embedded_reduced_'+pre_trained_model_name+'_'+str(isomap_dims)+'D.csv', index=False)\n",
    "\n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "            \n",
    "        else:\n",
    "            # import reduced embedding of targets\n",
    "            target_trans_df = pd.read_csv(target_embedding_reduced_path, sep=',')\n",
    "            \n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "\n",
    "        # plot embeddings if they are 2D\n",
    "        if isomap_dims ==2:\n",
    "            trans_df = pd.DataFrame(word_embeddings_transformed)\n",
    "            trans_df['target'] = target_df['target']\n",
    "            trans_df['sdg'] = target_df['sdg']\n",
    "            trans_df.plot.scatter(0,1,c='sdg', colormap='viridis') # colour by sdg\n",
    "            plt.title('Isomap 2D plot of text embedding')\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        # load pre-calculated embeddings\n",
    "        target_df = pd.read_csv(target_data_path, sep=';')\n",
    "        # get sentence list from target data\n",
    "        target_sentence_list = target_df['text'].tolist()\n",
    "        # get embeddings of targets\n",
    "        target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "        \n",
    "        # define source and target for KNN\n",
    "        Y = pd.DataFrame(target_embeddings)\n",
    "        Y['target'] = target_df['target']\n",
    "        Y['sdg'] = target_df['sdg']\n",
    "        X = pd.DataFrame(word_embeddings)\n",
    "        idx_ar = np.array(range(0,17), np.int64)\n",
    "        sdg_label_lst = []\n",
    "        for row in (np.array(sdg_predictions)==1):\n",
    "            sdg_label_lst.append(idx_ar[row])\n",
    "        X['sdg'] = sdg_label_lst\n",
    "        sdg_prob_lst = []\n",
    "        for row in sdg_raw_outputs:\n",
    "            sdg_prob_lst.append(row)\n",
    "        X['sdg_probability'] = sdg_prob_lst\n",
    "    \n",
    "    \n",
    "#     # create dictionary of knn models per sdg\n",
    "#     # train/prepare knn models\n",
    "#     knn = KNeighborsClassifier(n_neighbors=knn_n_val, weights='distance')\n",
    "#     knn_models_dict = {}\n",
    "#     for i in range(0+1, 17+1):\n",
    "#         Y_target_df = Y[Y['sdg']==i]\n",
    "#         Y_embedding_df = Y_target_df.loc[:, ~Y_target_df.columns.isin(['sdg', 'target'])]\n",
    "#         model = knn.fit(Y_embedding_df, Y_target_df['target'])\n",
    "#         knn_models_dict[i] = model\n",
    "        \n",
    "#     ############## save dict to disk and add option in function to load or train\n",
    "        \n",
    "#     # apply knn\n",
    "#     results_df = pd.DataFrame()\n",
    "#     for i,j in knn_models_dict.items():\n",
    "#         results_df['text'] = text_lst\n",
    "#         results_df['sdg'] = X['sdg']\n",
    "#         results_df['sdg_probability'] = X['sdg_probability']\n",
    "#         X_embedding_df = X.loc[:, ~X.columns.isin(['sdg', 'sdg_probability'])]\n",
    "#         results_df['sdg_'+str(i)+'_targets_prob'] = j.predict_proba(X_embedding_df).tolist()\n",
    "#         results_df['sdg_'+str(i)+'_targets_pred'] = j.predict(X_embedding_df).tolist()\n",
    "\n",
    "#################### use old cosine method and use parameter search to learn distance cutoff for each SDG?\n",
    "    # define final results table\n",
    "    results_df = pd.DataFrame()\n",
    "    results_df['text'] = text_lst\n",
    "    results_df['sdg'] = X['sdg']\n",
    "    results_df['sdg_probability'] = X['sdg_probability']\n",
    "    # calculate pairwise cosine similarity between targets and text list\n",
    "    similarity_ar = cosine_similarity(X.loc[:, ~X.columns.isin(['sdg', 'sdg_probability'])], \n",
    "                                  Y.loc[:, ~Y.columns.isin(['sdg', 'target'])],\n",
    "                                 )\n",
    "    results_df['target_similarity'] = similarity_ar.tolist()\n",
    "    # select targets on distance sdg and threshold\n",
    "    targets_ar = np.array(Y['target'])\n",
    "    targets_full_ar = np.tile(targets_ar, (len(d), 1)) # expand to make full column##########################\n",
    "    sdg_ar = np.array(Y['sdg'])\n",
    "    sdg_full_ar = np.tile(sdg_ar, (len(d), 1)) # expand to make full column##########################\n",
    "    \n",
    "    # select clasdified SDGs\n",
    "    sdg_onehot_ar = np.array([])\n",
    "    for i in len(sdg_full_ar):\n",
    "        isin_ar = np.isin(sdg_full_ar[i], sdg_select_ar[i])\n",
    "        sdg_onehot_ar = np.vstack((sdg_onehot_ar,isin_ar))\n",
    "        \n",
    "    target_label_ar = (similarity_ar>=target_threshold_val)*sdg_onehot_ar\n",
    "    results_df['target'] = target_label_ar\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6834437-148d-49b5-8948-8fb1718de944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T20:00:48.954197Z",
     "iopub.status.busy": "2022-08-30T20:00:48.953697Z",
     "iopub.status.idle": "2022-08-30T20:00:55.184671Z",
     "shell.execute_reply": "2022-08-30T20:00:55.182671Z",
     "shell.execute_reply.started": "2022-08-30T20:00:48.954197Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2ForTextRepresentation: ['h.11.attn.bias', 'h.1.attn.bias', 'h.7.mlp.c_proj.bias', 'h.8.ln_2.weight', 'ln_f.bias', 'h.8.attn.bias', 'h.6.attn.bias', 'h.3.ln_2.bias', 'h.9.attn.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.6.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.1.mlp.c_fc.bias', 'h.6.mlp.c_proj.bias', 'h.8.attn.c_attn.weight', 'h.2.ln_1.bias', 'h.1.ln_2.weight', 'h.2.ln_2.bias', 'h.2.ln_2.weight', 'h.6.mlp.c_proj.weight', 'h.5.ln_1.bias', 'h.11.ln_1.bias', 'h.1.mlp.c_fc.weight', 'h.5.ln_1.weight', 'h.5.ln_2.bias', 'h.0.attn.c_proj.weight', 'h.6.ln_1.bias', 'h.4.ln_2.weight', 'h.6.mlp.c_fc.weight', 'h.3.attn.c_attn.bias', 'h.8.mlp.c_fc.weight', 'h.1.ln_1.bias', 'h.1.attn.c_attn.bias', 'ln_f.weight', 'h.4.attn.c_proj.weight', 'h.4.ln_2.bias', 'h.3.mlp.c_proj.bias', 'h.7.attn.c_proj.weight', 'h.8.ln_1.weight', 'h.9.mlp.c_proj.bias', 'h.3.ln_2.weight', 'h.10.mlp.c_proj.weight', 'h.3.mlp.c_fc.bias', 'h.11.ln_2.weight', 'h.10.mlp.c_proj.bias', 'h.9.attn.c_attn.weight', 'h.7.attn.c_proj.bias', 'h.3.attn.c_proj.bias', 'h.5.attn.c_attn.weight', 'h.2.attn.c_attn.bias', 'h.4.ln_1.bias', 'wpe.weight', 'h.6.ln_1.weight', 'h.3.mlp.c_proj.weight', 'h.10.attn.bias', 'h.7.ln_1.bias', 'h.3.attn.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.8.mlp.c_fc.bias', 'h.2.attn.bias', 'h.1.ln_1.weight', 'h.10.ln_1.weight', 'h.2.mlp.c_proj.bias', 'h.8.ln_1.bias', 'h.1.ln_2.bias', 'h.0.ln_2.bias', 'h.7.attn.c_attn.weight', 'h.9.mlp.c_fc.weight', 'h.8.attn.c_attn.bias', 'h.11.mlp.c_fc.weight', 'h.0.mlp.c_fc.weight', 'h.11.ln_2.bias', 'h.8.ln_2.bias', 'h.10.attn.c_attn.weight', 'h.1.attn.c_proj.weight', 'h.10.attn.c_attn.bias', 'h.7.ln_2.weight', 'h.3.ln_1.weight', 'h.0.attn.c_attn.weight', 'h.11.mlp.c_proj.bias', 'h.3.mlp.c_fc.weight', 'h.4.mlp.c_proj.weight', 'h.9.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.4.attn.c_attn.bias', 'h.4.attn.bias', 'h.7.ln_2.bias', 'wte.weight', 'h.4.mlp.c_fc.weight', 'h.4.ln_1.weight', 'h.8.mlp.c_proj.bias', 'h.5.attn.c_proj.weight', 'h.0.attn.c_attn.bias', 'h.2.ln_1.weight', 'h.10.ln_2.bias', 'h.11.attn.c_proj.bias', 'h.11.mlp.c_proj.weight', 'h.5.mlp.c_fc.bias', 'h.0.ln_1.bias', 'h.10.ln_1.bias', 'h.7.mlp.c_proj.weight', 'h.10.mlp.c_fc.bias', 'h.5.attn.c_attn.bias', 'h.10.mlp.c_fc.weight', 'h.5.attn.bias', 'h.9.ln_1.bias', 'h.2.mlp.c_fc.weight', 'h.1.attn.c_proj.bias', 'h.9.attn.c_attn.bias', 'h.9.ln_2.weight', 'h.11.mlp.c_fc.bias', 'h.11.attn.c_attn.weight', 'h.6.attn.c_proj.weight', 'h.4.attn.c_attn.weight', 'h.8.attn.c_proj.weight', 'h.3.attn.bias', 'h.9.attn.bias', 'h.3.ln_1.bias', 'h.9.ln_1.weight', 'h.5.mlp.c_proj.bias', 'h.7.mlp.c_fc.weight', 'h.3.attn.c_attn.weight', 'h.10.attn.c_proj.bias', 'h.0.ln_2.weight', 'h.5.mlp.c_fc.weight', 'h.5.attn.c_proj.bias', 'h.6.attn.c_proj.bias', 'h.0.mlp.c_proj.bias', 'h.7.attn.c_attn.bias', 'h.2.attn.c_attn.weight', 'h.7.attn.bias', 'h.4.mlp.c_fc.bias', 'h.10.attn.c_proj.weight', 'h.11.attn.c_proj.weight', 'h.6.ln_2.bias', 'h.9.mlp.c_proj.weight', 'h.11.attn.c_attn.bias', 'h.11.ln_1.weight', 'h.7.mlp.c_fc.bias', 'h.6.attn.c_attn.bias', 'h.6.mlp.c_fc.bias', 'h.9.ln_2.bias', 'h.9.attn.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.5.ln_2.weight', 'h.2.attn.c_proj.weight', 'h.8.attn.c_proj.bias', 'h.1.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.8.mlp.c_proj.weight', 'h.0.attn.bias', 'h.6.attn.c_attn.weight', 'h.1.mlp.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.2.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.10.ln_2.weight', 'h.7.ln_1.weight', 'h.0.ln_1.weight']\n",
      "- This IS expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at gpt2 and are newly initialized: ['gpt2.h.3.attn.bias', 'gpt2.h.1.ln_2.bias', 'gpt2.h.9.ln_1.weight', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.wpe.weight', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.2.ln_2.weight', 'gpt2.h.10.ln_1.weight', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.9.ln_1.bias', 'gpt2.h.0.attn.masked_bias', 'gpt2.h.2.ln_1.bias', 'gpt2.h.1.ln_2.weight', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.0.ln_2.weight', 'gpt2.h.5.attn.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.3.ln_2.weight', 'gpt2.ln_f.bias', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.1.attn.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.9.attn.bias', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.3.attn.masked_bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.5.ln_2.weight', 'gpt2.h.8.ln_1.weight', 'gpt2.h.7.ln_1.weight', 'gpt2.h.6.ln_1.bias', 'gpt2.h.7.ln_2.weight', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.11.attn.bias', 'gpt2.ln_f.weight', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.10.ln_2.weight', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.8.ln_2.bias', 'gpt2.h.2.attn.bias', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.4.attn.bias', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.6.ln_1.weight', 'gpt2.h.7.ln_1.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.4.ln_1.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.2.ln_1.weight', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.1.ln_1.bias', 'gpt2.h.6.attn.bias', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.6.ln_2.weight', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.9.attn.masked_bias', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.9.ln_2.bias', 'gpt2.h.10.attn.masked_bias', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.7.attn.masked_bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.1.attn.masked_bias', 'gpt2.h.10.ln_2.bias', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.8.attn.bias', 'gpt2.h.8.ln_2.weight', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.5.attn.masked_bias', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.10.ln_1.bias', 'gpt2.h.11.ln_1.bias', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.wte.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.4.ln_1.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.4.attn.masked_bias', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.11.ln_1.weight', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.4.ln_2.bias', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.11.attn.masked_bias', 'gpt2.h.11.ln_2.bias', 'gpt2.h.10.attn.bias', 'gpt2.h.8.attn.masked_bias', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.3.ln_2.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.0.ln_1.bias', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.11.ln_2.weight', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.2.attn.masked_bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.0.attn.bias', 'gpt2.h.7.attn.bias', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.6.attn.masked_bias', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.9.ln_2.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.2.ln_2.bias', 'gpt2.h.5.ln_2.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.0.ln_2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "C:\\Users\\gcmar\\Desktop\\GIT_REPOS\\Transformers_simple_wandb_experiments\\SASDGHUB\\outputs\\best_model\\\\ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\configuration_utils.py:616\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 616\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\utils\\hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\utils\\hub.py:502\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    501\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mhead(url, headers\u001b[38;5;241m=\u001b[39mheaders, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout)\n\u001b[1;32m--> 502\u001b[0m \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m etag \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Linked-Etag\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# The repo was not found and the user is not Authenticated\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m401 Client Error: Repository not found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the repo is private, make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m     )\n\u001b[0;32m    422\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error: Repository not found for url: https://huggingface.co/C:%5CUsers%5Cgcmar%5CDesktop%5CGIT_REPOS%5CTransformers_simple_wandb_experiments%5CSASDGHUB%5Coutputs%5Cbest_model%5C%5C/resolve/main/config.json. If the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a\u001b[38;5;241m=\u001b[39m \u001b[43mclassify_sdg_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_isomap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m a\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mclassify_sdg_target\u001b[1;34m(text_lst, target_data_path, run_isomap, target_embedding_reduced_path, isomap_dims, isomap_neigbors, pre_trained_model_type, pre_trained_model_name, target_threshold_val)\u001b[0m\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m RepresentationModel(\n\u001b[0;32m     19\u001b[0m     pre_trained_model_type,\n\u001b[0;32m     20\u001b[0m     pre_trained_model_name, \u001b[38;5;66;03m#gpt2 , gpt2-large\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mmodel_args,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# classify sdg of text\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m sdg_predictions, sdg_raw_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_sdg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_lst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# get embeddings of text\u001b[39;00m\n\u001b[0;32m     27\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_sentences(text_lst, combine_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mclassify_sdg\u001b[1;34m(text_lst)\u001b[0m\n\u001b[0;32m      4\u001b[0m cuda_available \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# import model from path (this path is the directory with all the model files)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m sdg_model \u001b[38;5;241m=\u001b[39m \u001b[43mMultiLabelClassificationModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgcmar\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mGIT_REPOS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTransformers_simple_wandb_experiments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSASDGHUB\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbest_model\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_available\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m predictions, raw_outputs \u001b[38;5;241m=\u001b[39m sdg_model\u001b[38;5;241m.\u001b[39mpredict(text_lst)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(predictions, raw_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\simpletransformers\\classification\\multi_label_classification_model.py:204\u001b[0m, in \u001b[0;36mMultiLabelClassificationModel.__init__\u001b[1;34m(self, model_type, model_name, num_labels, pos_weight, args, use_cuda, cuda_device, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m config_class, model_class, tokenizer_class \u001b[38;5;241m=\u001b[39m MODEL_CLASSES[model_type]\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_labels:\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    205\u001b[0m         model_name, num_labels\u001b[38;5;241m=\u001b[39mnum_labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    206\u001b[0m     )\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m num_labels\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\configuration_utils.py:534\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrainedConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[0;32m    536\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    537\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    538\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\configuration_utils.py:561\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_files\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_path(\n\u001b[0;32m    617\u001b[0m         config_file,\n\u001b[0;32m    618\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m--> 628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier listed on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token having \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpermission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    633\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists for this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel name. Check the model page at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailable revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    639\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: C:\\Users\\gcmar\\Desktop\\GIT_REPOS\\Transformers_simple_wandb_experiments\\SASDGHUB\\outputs\\best_model\\\\ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "a= classify_sdg_target(data.iloc[-10:,:]['abstract'].tolist(), run_isomap=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d90be-4681-42dd-982e-9e8a7ee2688a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-30T19:31:55.180469Z",
     "iopub.status.idle": "2022-08-30T19:31:55.180969Z",
     "shell.execute_reply": "2022-08-30T19:31:55.180969Z",
     "shell.execute_reply.started": "2022-08-30T19:31:55.180969Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create evaluation function for target function\n",
    "# create sweep for parameter tuning with wandb with evaluation and training fucntions\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02052a80-7128-47de-8902-06f3d9406a0c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-30T19:31:55.182468Z",
     "iopub.status.idle": "2022-08-30T19:31:55.182968Z",
     "shell.execute_reply": "2022-08-30T19:31:55.182968Z",
     "shell.execute_reply.started": "2022-08-30T19:31:55.182968Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modify function to be used for testing both sdg and target classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
