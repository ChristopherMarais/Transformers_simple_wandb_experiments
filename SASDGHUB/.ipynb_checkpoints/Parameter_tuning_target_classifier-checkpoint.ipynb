{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fd34e1-4700-417a-b95f-42fd97e908b1",
   "metadata": {},
   "source": [
    "### Target classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689e0297-9b06-429a-aeeb-7741fcefbc9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:07:19.853370Z",
     "iopub.status.busy": "2022-08-30T02:07:19.853370Z",
     "iopub.status.idle": "2022-08-30T02:07:24.438104Z",
     "shell.execute_reply": "2022-08-30T02:07:24.437103Z",
     "shell.execute_reply.started": "2022-08-30T02:07:19.853370Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7886bd55-872c-483c-a0c4-42ae8765d49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:07:24.440092Z",
     "iopub.status.busy": "2022-08-30T02:07:24.439093Z",
     "iopub.status.idle": "2022-08-30T02:07:25.160094Z",
     "shell.execute_reply": "2022-08-30T02:07:25.159103Z",
     "shell.execute_reply.started": "2022-08-30T02:07:24.440092Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('OneHot_Combined_cln_utf8.tsv', sep='\\t')\n",
    "data = data[data['source']!='SASDG_Hub'] #keep the articles classified by Willem separate as an unseen testing set\n",
    "data = data.iloc[-1000:,:] # select a small subset of the data (last 1000 rows)\n",
    "\n",
    "# import target data\n",
    "target_df = pd.read_csv('Targets.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1160642-d5b4-4ed4-9461-a49a8557e658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:07:25.161078Z",
     "iopub.status.busy": "2022-08-30T02:07:25.161078Z",
     "iopub.status.idle": "2022-08-30T02:07:25.176101Z",
     "shell.execute_reply": "2022-08-30T02:07:25.175103Z",
     "shell.execute_reply.started": "2022-08-30T02:07:25.161078Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to apply model to text\n",
    "def classify_sdg(text_lst):\n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    # import model from path (this path is the directory with all the model files)\n",
    "    sdg_model = MultiLabelClassificationModel(\n",
    "            \"xlnet\",\n",
    "            r\"C:\\Users\\gcmar\\Desktop\\GIT_REPOS\\Transformers_simple_wandb_experiments\\SASDGHUB\\outputs\\best_model\\\\\",\n",
    "            num_labels=17,\n",
    "            use_cuda=cuda_available,\n",
    "            )\n",
    "    predictions, raw_outputs = sdg_model.predict(text_lst)\n",
    "    return(predictions, raw_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e9d699-4603-46fa-8e6f-1598361379e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:07:25.178086Z",
     "iopub.status.busy": "2022-08-30T02:07:25.178086Z",
     "iopub.status.idle": "2022-08-30T02:07:25.206659Z",
     "shell.execute_reply": "2022-08-30T02:07:25.205669Z",
     "shell.execute_reply.started": "2022-08-30T02:07:25.178086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to classify targets\n",
    "def classify_sdg_target(text_lst, \n",
    "                        target_data_path='Targets.csv',\n",
    "                        run_isomap=True, # run faster for multiple samples otherwise (target_data_path required when this is False) (do not use when only one sample)\n",
    "                        target_embedding_reduced_path=None, # load a previously calculated and reduced embedding for the targets 'outputs/targets_embedded_reduced_gpt2_2D.csv'\n",
    "                        isomap_dims = 2,\n",
    "                        isomap_neigbors = 5, # has to be <= len(text_lst) a.k.a n_samples\n",
    "                        pre_trained_model_type='gpt2', \n",
    "                        pre_trained_model_name='gpt2',\n",
    "                        knn_n_val=3):\n",
    "    \n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "\n",
    "    # define and load model from hugging face\n",
    "    model_args = ModelArgs(max_seq_length=1024)\n",
    "    # model import\n",
    "    model = RepresentationModel(\n",
    "        pre_trained_model_type,\n",
    "        pre_trained_model_name, #gpt2 , gpt2-large\n",
    "        args=model_args,\n",
    "    )\n",
    "    \n",
    "    # classify sdg of text\n",
    "    sdg_predictions, sdg_raw_outputs = classify_sdg(text_lst)\n",
    "    # get embeddings of text\n",
    "    word_embeddings = model.encode_sentences(text_lst, combine_strategy=\"mean\")\n",
    "    \n",
    "    \n",
    "    # ISOMAP\n",
    "    if run_isomap==True: \n",
    "        # reduce isomap_neigbors to fit the number of samples\n",
    "        n_samples = len(text_lst)\n",
    "        if n_samples < isomap_neigbors:\n",
    "            isomap_neigbors = np.max([1,n_samples-1])\n",
    "            print('Reduced isomap_n_neigbors to: ', isomap_neigbors)\n",
    "\n",
    "        # reduce dimensions of embeddings to 2 (can be reduced to higher dimensions)\n",
    "        isomap = Isomap(n_components=isomap_dims, n_neighbors=isomap_neigbors-1) # input is an array with samples x features\n",
    "        word_embeddings_transformed = isomap.fit_transform(word_embeddings)\n",
    "\n",
    "        if target_embedding_reduced_path == None:\n",
    "            # load pre-calculated embeddings\n",
    "            target_df = pd.read_csv(target_data_path, sep=';')\n",
    "            # get sentence list from target data\n",
    "            target_sentence_list = target_df['text'].tolist()\n",
    "            # get embeddings of targets\n",
    "            target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "            target_embeddings_transformed = isomap.fit_transform(target_embeddings)\n",
    "\n",
    "            # add labels to reduced embeddings\n",
    "            target_trans_df = pd.DataFrame(target_embeddings_transformed)\n",
    "            target_trans_df['target'] = target_df['target']\n",
    "            target_trans_df['sdg'] = target_df['sdg']\n",
    "            target_trans_df.to_csv('outputs/targets_embedded_reduced_'+pre_trained_model_name+'_'+str(isomap_dims)+'D.csv', index=False)\n",
    "\n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "            \n",
    "        else:\n",
    "            # import reduced embedding of targets\n",
    "            target_trans_df = pd.read_csv(target_embedding_reduced_path, sep=',')\n",
    "            \n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "\n",
    "        # plot embeddings if they are 2D\n",
    "        if isomap_dims ==2:\n",
    "            trans_df = pd.DataFrame(word_embeddings_transformed)\n",
    "            trans_df['target'] = target_df['target']\n",
    "            trans_df['sdg'] = target_df['sdg']\n",
    "            trans_df.plot.scatter(0,1,c='sdg', colormap='viridis') # colour by sdg\n",
    "            plt.title('Isomap 2D plot of text embedding')\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        # load pre-calculated embeddings\n",
    "        target_df = pd.read_csv(target_data_path, sep=';')\n",
    "        # get sentence list from target data\n",
    "        target_sentence_list = target_df['text'].tolist()\n",
    "        # get embeddings of targets\n",
    "        target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "        \n",
    "        # define source and target for KNN\n",
    "        Y = pd.DataFrame(target_embeddings)\n",
    "        Y['target'] = target_df['target']\n",
    "        Y['sdg'] = target_df['sdg']\n",
    "        X = pd.DataFrame(word_embeddings)\n",
    "        idx_ar = np.array(range(0,17), np.int64)\n",
    "        sdg_label_lst = []\n",
    "        for row in (np.array(sdg_predictions)==1):\n",
    "            sdg_label_lst.append(idx_ar[row])\n",
    "        X['sdg'] = sdg_label_lst\n",
    "        sdg_prob_lst = []\n",
    "        for row in sdg_raw_outputs:\n",
    "            sdg_prob_lst.append(row)\n",
    "        X['sdg_probability'] = sdg_prob_lst\n",
    "    \n",
    "    \n",
    "    # create dictionary of knn models per sdg\n",
    "    # train/prepare knn models\n",
    "    knn = KNeighborsClassifier(n_neighbors=knn_n_val, weights='distance')\n",
    "    knn_models_dict = {}\n",
    "    for i in range(0+1, 17+1):\n",
    "        Y_target_df = Y[Y['sdg']==i]\n",
    "        Y_embedding_df = Y_target_df.loc[:, ~Y_target_df.columns.isin(['sdg', 'target'])]\n",
    "        model = knn.fit(Y_embedding_df, Y_target_df['target'])\n",
    "        knn_models_dict[i] = model.copy()\n",
    "        \n",
    "    ############## save dict to disk and add option in function to load or train\n",
    "        \n",
    "    # apply knn\n",
    "    results_df = pd.DataFrame()\n",
    "    for i,j in knn_models_dict.items():\n",
    "        results_df['text'] = text_lst\n",
    "        results_df['sdg'] = X['sdg']\n",
    "        results_df['sdg_probability'] = X['sdg_probability']\n",
    "        X_embedding_df = X.loc[:, ~X.columns.isin(['sdg', 'sdg_probability'])]\n",
    "        results_df['sdg_'+str(i)+'_targets_prob'] = j.predict_proba(X_embedding_df).tolist()\n",
    "        results_df['sdg_'+str(i)+'_targets_pred'] = j.predict(X_embedding_df).tolist()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598d90be-4681-42dd-982e-9e8a7ee2688a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:07:25.208661Z",
     "iopub.status.busy": "2022-08-30T02:07:25.207667Z",
     "iopub.status.idle": "2022-08-30T02:07:25.222685Z",
     "shell.execute_reply": "2022-08-30T02:07:25.221662Z",
     "shell.execute_reply.started": "2022-08-30T02:07:25.208661Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create evaluation function for target function\n",
    "# create sweep for parameter tuning with wandb with evaluation and training fucntions\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6834437-148d-49b5-8948-8fb1718de944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:07:25.223667Z",
     "iopub.status.busy": "2022-08-30T02:07:25.223667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2ForTextRepresentation: ['h.2.attn.c_proj.bias', 'h.3.attn.c_proj.weight', 'h.2.mlp.c_fc.bias', 'h.5.attn.c_proj.bias', 'h.4.mlp.c_fc.weight', 'h.6.attn.c_proj.weight', 'h.8.mlp.c_fc.weight', 'h.7.ln_1.weight', 'h.11.attn.c_proj.weight', 'h.4.ln_2.bias', 'h.7.ln_2.bias', 'h.10.attn.bias', 'h.5.ln_2.weight', 'h.8.ln_1.weight', 'h.6.mlp.c_fc.weight', 'h.5.mlp.c_fc.weight', 'h.1.ln_2.bias', 'h.7.attn.c_attn.bias', 'h.7.attn.bias', 'h.3.ln_2.bias', 'h.9.mlp.c_proj.bias', 'h.5.attn.bias', 'h.6.ln_2.bias', 'h.11.mlp.c_proj.weight', 'h.7.mlp.c_fc.weight', 'h.6.ln_1.bias', 'h.11.mlp.c_fc.bias', 'h.6.ln_2.weight', 'h.7.mlp.c_proj.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.1.attn.c_attn.bias', 'h.3.ln_1.weight', 'h.8.attn.c_proj.weight', 'h.10.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.8.mlp.c_fc.bias', 'h.4.attn.c_attn.bias', 'h.10.attn.c_attn.weight', 'h.10.mlp.c_fc.bias', 'h.1.attn.c_proj.weight', 'h.2.attn.c_attn.weight', 'h.4.ln_2.weight', 'wte.weight', 'h.11.attn.c_attn.weight', 'h.5.attn.c_attn.weight', 'h.3.ln_2.weight', 'h.8.ln_2.weight', 'h.5.mlp.c_fc.bias', 'h.4.attn.c_attn.weight', 'h.10.attn.c_proj.bias', 'h.7.attn.c_proj.weight', 'h.10.attn.c_proj.weight', 'h.11.mlp.c_proj.bias', 'wpe.weight', 'h.3.mlp.c_proj.bias', 'h.0.attn.c_proj.bias', 'h.10.mlp.c_proj.bias', 'h.5.ln_1.bias', 'h.2.mlp.c_proj.weight', 'h.9.attn.bias', 'h.3.mlp.c_fc.bias', 'h.11.ln_1.bias', 'h.6.attn.c_attn.weight', 'h.1.ln_2.weight', 'h.8.ln_1.bias', 'h.3.attn.c_attn.bias', 'h.4.ln_1.weight', 'h.4.ln_1.bias', 'h.8.attn.c_proj.bias', 'h.8.mlp.c_proj.weight', 'h.9.ln_2.weight', 'h.8.mlp.c_proj.bias', 'h.10.ln_2.bias', 'h.6.mlp.c_fc.bias', 'h.1.ln_1.bias', 'h.3.attn.bias', 'h.11.attn.c_proj.bias', 'h.6.attn.c_proj.bias', 'h.6.attn.c_attn.bias', 'h.0.mlp.c_fc.weight', 'h.8.attn.c_attn.bias', 'h.10.ln_2.weight', 'h.1.ln_1.weight', 'h.1.attn.bias', 'h.9.mlp.c_proj.weight', 'h.1.attn.c_attn.weight', 'h.9.attn.c_attn.bias', 'h.2.attn.bias', 'h.0.attn.c_attn.weight', 'h.9.ln_1.bias', 'h.3.mlp.c_proj.weight', 'h.2.ln_1.weight', 'h.1.attn.c_proj.bias', 'h.2.ln_2.bias', 'h.0.attn.c_proj.weight', 'h.2.attn.c_proj.weight', 'h.3.ln_1.bias', 'h.5.attn.c_attn.bias', 'h.3.attn.c_attn.weight', 'h.5.mlp.c_proj.bias', 'h.7.mlp.c_proj.weight', 'h.10.attn.c_attn.bias', 'h.1.mlp.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.0.attn.bias', 'h.1.mlp.c_fc.bias', 'h.6.mlp.c_proj.bias', 'h.3.mlp.c_fc.weight', 'h.2.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.3.attn.c_proj.bias', 'h.11.attn.bias', 'h.7.ln_2.weight', 'h.8.attn.bias', 'h.9.mlp.c_fc.bias', 'h.6.attn.bias', 'h.11.ln_2.weight', 'ln_f.bias', 'h.4.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.9.ln_1.weight', 'h.11.attn.c_attn.bias', 'h.0.ln_1.bias', 'h.2.ln_2.weight', 'h.6.mlp.c_proj.weight', 'h.5.ln_2.bias', 'h.4.mlp.c_proj.bias', 'h.8.attn.c_attn.weight', 'h.0.ln_2.bias', 'h.1.mlp.c_fc.weight', 'h.8.ln_2.bias', 'h.11.ln_2.bias', 'h.0.mlp.c_proj.bias', 'h.5.ln_1.weight', 'ln_f.weight', 'h.9.attn.c_attn.weight', 'h.9.attn.c_proj.weight', 'h.10.ln_1.bias', 'h.1.mlp.c_proj.bias', 'h.9.attn.c_proj.bias', 'h.2.ln_1.bias', 'h.2.attn.c_attn.bias', 'h.5.attn.c_proj.weight', 'h.0.ln_1.weight', 'h.4.attn.c_proj.weight', 'h.9.ln_2.bias', 'h.10.ln_1.weight', 'h.7.ln_1.bias', 'h.4.mlp.c_proj.weight', 'h.7.attn.c_attn.weight', 'h.4.attn.bias', 'h.9.mlp.c_fc.weight', 'h.0.mlp.c_proj.weight', 'h.2.mlp.c_fc.weight', 'h.0.attn.c_attn.bias', 'h.10.mlp.c_proj.weight', 'h.6.ln_1.weight', 'h.11.mlp.c_fc.weight', 'h.7.attn.c_proj.bias']\n",
      "- This IS expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at gpt2 and are newly initialized: ['gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.3.attn.masked_bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.0.attn.masked_bias', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.4.ln_1.bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.8.attn.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.3.ln_2.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.ln_f.weight', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.11.ln_2.bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.4.ln_1.weight', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.wte.weight', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.8.ln_2.weight', 'gpt2.h.7.attn.bias', 'gpt2.h.11.attn.bias', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.1.ln_1.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.6.ln_1.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.8.attn.masked_bias', 'gpt2.h.2.ln_1.bias', 'gpt2.h.2.ln_1.weight', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.6.ln_1.bias', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.9.ln_2.weight', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.7.ln_2.weight', 'gpt2.h.4.ln_2.bias', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.7.attn.masked_bias', 'gpt2.h.5.ln_2.weight', 'gpt2.h.6.attn.masked_bias', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.4.attn.masked_bias', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.10.attn.masked_bias', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.5.attn.masked_bias', 'gpt2.h.1.ln_2.weight', 'gpt2.h.7.ln_1.bias', 'gpt2.h.9.attn.masked_bias', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.1.ln_2.bias', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.7.ln_1.weight', 'gpt2.h.2.attn.masked_bias', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.1.attn.bias', 'gpt2.h.0.attn.bias', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.6.ln_2.bias', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.2.ln_2.weight', 'gpt2.h.0.ln_2.bias', 'gpt2.h.2.ln_2.bias', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.11.ln_1.weight', 'gpt2.h.6.ln_2.weight', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.0.ln_1.bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.2.attn.bias', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.4.attn.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.ln_f.bias', 'gpt2.h.11.ln_1.bias', 'gpt2.h.11.attn.masked_bias', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.10.ln_1.bias', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.6.attn.bias', 'gpt2.h.8.ln_1.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.9.attn.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.5.attn.bias', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.10.ln_2.weight', 'gpt2.h.10.ln_2.bias', 'gpt2.h.1.attn.masked_bias', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.9.ln_2.bias', 'gpt2.h.3.ln_2.weight', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.9.ln_1.bias', 'gpt2.wpe.weight', 'gpt2.h.8.ln_2.bias', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.3.ln_1.bias', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.9.ln_1.weight', 'gpt2.h.3.attn.bias', 'gpt2.h.5.ln_2.bias', 'gpt2.h.10.ln_1.weight', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.0.ln_2.weight', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.10.attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db06c7ea8c5b42beaf3942e8ef06534c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd050b772dd34a9dad1c4165d7cd9a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = classify_sdg_target(data.iloc[-10:,:]['abstract'].tolist(), run_isomap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52b3b4-4f73-4ca4-92d0-abaa95bd05d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02052a80-7128-47de-8902-06f3d9406a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modify function to be used for testing both sdg and target classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
