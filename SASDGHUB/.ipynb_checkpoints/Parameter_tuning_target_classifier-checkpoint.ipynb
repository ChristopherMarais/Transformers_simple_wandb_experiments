{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fd34e1-4700-417a-b95f-42fd97e908b1",
   "metadata": {},
   "source": [
    "### Target classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689e0297-9b06-429a-aeeb-7741fcefbc9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T00:14:13.026565Z",
     "iopub.status.busy": "2022-09-01T00:14:13.026065Z",
     "iopub.status.idle": "2022-09-01T00:14:19.379702Z",
     "shell.execute_reply": "2022-09-01T00:14:19.378701Z",
     "shell.execute_reply.started": "2022-09-01T00:14:13.026565Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7886bd55-872c-483c-a0c4-42ae8765d49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T00:14:19.382204Z",
     "iopub.status.busy": "2022-09-01T00:14:19.382204Z",
     "iopub.status.idle": "2022-09-01T00:14:20.216701Z",
     "shell.execute_reply": "2022-09-01T00:14:20.215701Z",
     "shell.execute_reply.started": "2022-09-01T00:14:19.382204Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('OneHot_Combined_cln_utf8.tsv', sep='\\t')\n",
    "data = data[data['source']!='SASDG_Hub'] #keep the articles classified by Willem separate as an unseen testing set\n",
    "# data = data.iloc[-1000:,:] # select a small subset of the data (last 1000 rows)\n",
    "\n",
    "# import target data\n",
    "target_df = pd.read_csv('Targets.csv', sep=';')\n",
    "targets_lst = target_df['target'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1160642-d5b4-4ed4-9461-a49a8557e658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T00:14:20.218702Z",
     "iopub.status.busy": "2022-09-01T00:14:20.218203Z",
     "iopub.status.idle": "2022-09-01T00:14:20.233202Z",
     "shell.execute_reply": "2022-09-01T00:14:20.231204Z",
     "shell.execute_reply.started": "2022-09-01T00:14:20.218702Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to apply model to text\n",
    "def classify_sdg(text_lst):\n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    # import model from path (this path is the directory with all the model files)\n",
    "    sdg_model = MultiLabelClassificationModel(\n",
    "            \"xlnet\",\n",
    "            \"outputs/best_model/\",   #os.getcwd()+\"\\\\outputs\\\\best_model\\\\\", #C:\\Users\\GCM\\Desktop\\GIT_REPOS\\Transformers_simple_wandb_experiments\\SASDGHUB\\\n",
    "            num_labels=17,\n",
    "            use_cuda=cuda_available,\n",
    "            )\n",
    "    predictions, raw_outputs = sdg_model.predict(text_lst)\n",
    "    return(predictions, raw_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e9d699-4603-46fa-8e6f-1598361379e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T00:14:20.235203Z",
     "iopub.status.busy": "2022-09-01T00:14:20.234704Z",
     "iopub.status.idle": "2022-09-01T00:14:20.741461Z",
     "shell.execute_reply": "2022-09-01T00:14:20.740461Z",
     "shell.execute_reply.started": "2022-09-01T00:14:20.235203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to classify targets\n",
    "def classify_sdg_target(text_lst, \n",
    "                        target_data_path='Targets.csv',\n",
    "                        run_isomap=True, # run faster for multiple samples otherwise (target_data_path required when this is False) (do not use when only one sample)\n",
    "                        target_embedding_reduced_path=None, # load a previously calculated and reduced embedding for the targets 'outputs/targets_embedded_reduced_gpt2_2D.csv'\n",
    "                        isomap_dims = 2,\n",
    "                        isomap_neigbors = 5, # has to be <= len(text_lst) a.k.a n_samples\n",
    "                        pre_trained_model_type='gpt2', \n",
    "                        pre_trained_model_name='gpt2',\n",
    "                        target_threshold_val=0.5):\n",
    "    \n",
    "    # see GPU avaialability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "\n",
    "    # define and load model from hugging face\n",
    "    model_args = ModelArgs(max_seq_length=1024)\n",
    "    # model import\n",
    "    model = RepresentationModel(\n",
    "        pre_trained_model_type,\n",
    "        pre_trained_model_name, #gpt2 , gpt2-large\n",
    "        args=model_args,\n",
    "    )\n",
    "    \n",
    "    # classify sdg of text\n",
    "    sdg_predictions, sdg_raw_outputs = classify_sdg(text_lst)\n",
    "    # get embeddings of text\n",
    "    word_embeddings = model.encode_sentences(text_lst, combine_strategy=\"mean\")\n",
    "    \n",
    "    \n",
    "    # ISOMAP\n",
    "    if run_isomap==True: \n",
    "        # reduce isomap_neigbors to fit the number of samples\n",
    "        n_samples = len(text_lst)\n",
    "        if n_samples < isomap_neigbors:\n",
    "            isomap_neigbors = np.max([1,n_samples-1])\n",
    "            print('Reduced isomap_n_neigbors to: ', isomap_neigbors)\n",
    "\n",
    "        # reduce dimensions of embeddings to 2 (can be reduced to higher dimensions)\n",
    "        isomap = Isomap(n_components=isomap_dims, n_neighbors=isomap_neigbors-1) # input is an array with samples x features\n",
    "        word_embeddings_transformed = isomap.fit_transform(word_embeddings)\n",
    "\n",
    "        if target_embedding_reduced_path == None:\n",
    "            # load pre-calculated embeddings\n",
    "            target_df = pd.read_csv(target_data_path, sep=';')\n",
    "            # get sentence list from target data\n",
    "            target_sentence_list = target_df['text'].tolist()\n",
    "            # get embeddings of targets\n",
    "            target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "            target_embeddings_transformed = isomap.fit_transform(target_embeddings)\n",
    "\n",
    "            # add labels to reduced embeddings\n",
    "            target_trans_df = pd.DataFrame(target_embeddings_transformed)\n",
    "            target_trans_df['target'] = target_df['target']\n",
    "            target_trans_df['sdg'] = target_df['sdg']\n",
    "            target_trans_df.to_csv('outputs/targets_embedded_reduced_'+pre_trained_model_name+'_'+str(isomap_dims)+'D.csv', index=False)\n",
    "\n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "            \n",
    "        else:\n",
    "            # import reduced embedding of targets\n",
    "            target_trans_df = pd.read_csv(target_embedding_reduced_path, sep=',')\n",
    "            \n",
    "            # define source and target for KNN\n",
    "            Y = target_trans_df\n",
    "            X = pd.DataFrame(word_embeddings_transformed)\n",
    "            idx_ar = np.array(range(0,17), np.int64)\n",
    "            sdg_label_lst = []\n",
    "            for row in (np.array(sdg_predictions)==1):\n",
    "                sdg_label_lst.append(idx_ar[row])\n",
    "            X['sdg'] = sdg_label_lst\n",
    "            sdg_prob_lst = []\n",
    "            for row in sdg_raw_outputs:\n",
    "                sdg_prob_lst.append(row)\n",
    "            X['sdg_probability'] = sdg_prob_lst\n",
    "\n",
    "        # plot embeddings if they are 2D\n",
    "        if isomap_dims ==2:\n",
    "            trans_df = pd.DataFrame(word_embeddings_transformed)\n",
    "            trans_df['target'] = target_df['target']\n",
    "            trans_df['sdg'] = target_df['sdg']\n",
    "            trans_df.plot.scatter(0,1,c='sdg', colormap='viridis') # colour by sdg\n",
    "            plt.title('Isomap 2D plot of text embedding')\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        # load pre-calculated embeddings\n",
    "        target_df = pd.read_csv(target_data_path, sep=';')\n",
    "        # get sentence list from target data\n",
    "        target_sentence_list = target_df['text'].tolist()\n",
    "        # get embeddings of targets\n",
    "        target_embeddings = model.encode_sentences(target_sentence_list, combine_strategy=\"mean\")\n",
    "        \n",
    "        # define source and target for KNN\n",
    "        Y = pd.DataFrame(target_embeddings)\n",
    "        Y['target'] = target_df['target']\n",
    "        Y['sdg'] = target_df['sdg']\n",
    "        X = pd.DataFrame(word_embeddings)\n",
    "        idx_ar = np.array(range(0,17), np.int64)\n",
    "        sdg_label_lst = []\n",
    "        for row in (np.array(sdg_predictions)==1):\n",
    "            sdg_label_lst.append(idx_ar[row])\n",
    "        X['sdg'] = sdg_label_lst\n",
    "        sdg_prob_lst = []\n",
    "        for row in sdg_raw_outputs:\n",
    "            sdg_prob_lst.append(row)\n",
    "        X['sdg_probability'] = sdg_prob_lst\n",
    "    \n",
    "    # use cosine similarity Kmeans variant to classify targets\n",
    "    # define final results table\n",
    "    results_df = pd.DataFrame()\n",
    "    results_df['text'] = text_lst\n",
    "    results_df['sdg'] = X['sdg']\n",
    "    results_df['sdg_probability'] = X['sdg_probability']\n",
    "    # calculate pairwise cosine similarity between targets and text list\n",
    "    similarity_ar = cosine_similarity(X.loc[:, ~X.columns.isin(['sdg', 'sdg_probability'])], \n",
    "                                  Y.loc[:, ~Y.columns.isin(['sdg', 'target'])],\n",
    "                                 )\n",
    "    results_df['target_similarity'] = similarity_ar.tolist()\n",
    "    # select targets on distance sdg and threshold\n",
    "    targets_ar = np.array(Y['target'])\n",
    "    targets_full_ar = np.tile(targets_ar, (len(text_lst), 1)) \n",
    "    sdg_ar = np.array(Y['sdg'])\n",
    "    sdg_full_ar = np.tile(sdg_ar, (len(text_lst), 1))\n",
    "    sdg_select_ar = np.array(X['sdg'])\n",
    "    \n",
    "    # select classified SDGs\n",
    "    sdg_onehot_lst = []\n",
    "    for i in range(len(sdg_full_ar)):\n",
    "        isin_ar = np.isin(sdg_full_ar[i], sdg_select_ar[i])\n",
    "        sdg_onehot_lst.append(isin_ar)\n",
    "    sdg_onehot_ar = np.vstack(sdg_onehot_lst)\n",
    "    target_onehot_ar = (similarity_ar>=target_threshold_val)*sdg_onehot_ar\n",
    "    target_label_lst = (targets_full_ar*target_onehot_ar).tolist()\n",
    "    results_df['target'] = [[ele for ele in sub if ele != ''] for sub in target_label_lst]\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a510ed7c-994a-4ae9-9793-a90d6c452f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T00:14:20.743467Z",
     "iopub.status.busy": "2022-09-01T00:14:20.742966Z",
     "iopub.status.idle": "2022-09-01T00:14:21.158088Z",
     "shell.execute_reply": "2022-09-01T00:14:21.157087Z",
     "shell.execute_reply.started": "2022-09-01T00:14:20.743467Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "# create functions for additional evaluation outputs\n",
    "def acc_result(true, pred):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    acc_sum = 0\n",
    "    for i in range(true.shape[0]):\n",
    "        acc_sum += sklearn.metrics.accuracy_score(true[i], pred[i])    \n",
    "    acc = acc_sum/true.shape[0]\n",
    "    return acc\n",
    "\n",
    "def f1_macro_result(true, pred):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    f1 = sklearn.metrics.f1_score(true, pred, average='samples')\n",
    "    return f1\n",
    "\n",
    "# def cm_wandb_result(true, pred):\n",
    "#     # modify labels and fill all combinations to use wand multiclass confusion matrix visually\n",
    "#     d=true-pred\n",
    "#     t_d = (d==1)\n",
    "#     p_d = (d==-1)\n",
    "#     n_d = (d==0)\n",
    "#     idx_ar = np.array(range(0,true.shape[1]))\n",
    "#     idx = np.tile(idx_ar,300).reshape(300,true.shape[1])\n",
    "#     n_labels = idx[n_d]\n",
    "#     t_lst = []\n",
    "#     for row in t_d:\n",
    "#         if row.sum()==0:\n",
    "#             t_lst.append(idx_ar)\n",
    "#         else:\n",
    "#             t_lst.append(idx_ar[row])\n",
    "#     p_lst = []\n",
    "#     for row in p_d:\n",
    "#         if row.sum()==0:\n",
    "#             p_lst.append(idx_ar)\n",
    "#         else:\n",
    "#             p_lst.append(idx_ar[row])\n",
    "#     for i in range(len(t_lst)):\n",
    "#         fill_ar = np.array(list(itertools.product(p_lst[i], t_lst[i])))\n",
    "#         t_labels = np.append(n_labels,fill_ar[:,0].tolist())\n",
    "#         p_labels = np.append(n_labels,fill_ar[:,1].tolist())\n",
    "#     wandb_cm = wandb.plot.confusion_matrix(probs=None, y_true=t_labels, preds=p_labels, class_names=sdg_lst)\n",
    "#     return wandb_cm\n",
    "\n",
    "def cm_result(true, pred):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    cm = sklearn.metrics.multilabel_confusion_matrix(true, pred)\n",
    "    return cm\n",
    "\n",
    "def cm_avg_result(true, pred):\n",
    "    one_hot = MultiLabelBinarizer(classes=targets_lst)\n",
    "    pred=one_hot.fit_transform(pred)\n",
    "    true=one_hot.fit_transform(true)\n",
    "    cm = sklearn.metrics.multilabel_confusion_matrix(true, pred)\n",
    "    cm_avg = cm.sum(axis=0)/true.shape[1]\n",
    "    return cm_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6834437-148d-49b5-8948-8fb1718de944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T00:16:05.798643Z",
     "iopub.status.busy": "2022-09-01T00:16:05.798143Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use all the data that have targets to estimate accuracy of the \n",
    "test_data = data[data['target'].notna()][['abstract', 'target']].iloc[-1000:,:].reset_index(drop=True)\n",
    "\n",
    "results_df = classify_sdg_target(text_lst=test_data['abstract'].tolist(),\n",
    "                                 target_data_path='Targets.csv',\n",
    "                                 run_isomap=True, # run faster for multiple samples otherwise (target_data_path required when this is False) (do not use when only one sample)\n",
    "                                 target_embedding_reduced_path=None, # load a previously calculated and reduced embedding for the targets 'outputs/targets_embedded_reduced_gpt2_2D.csv'\n",
    "                                 isomap_dims = 2,\n",
    "                                 isomap_neigbors = 5, # has to be <= len(text_lst) a.k.a n_samples\n",
    "                                 pre_trained_model_type='gpt2', \n",
    "                                 pre_trained_model_name='gpt2',\n",
    "                                 target_threshold_val=0.5\n",
    "                                )\n",
    "\n",
    "acc = acc_result(true=test_data['target'], pred=results_df['target'])\n",
    "f1 = f1_macro_result(true=test_data['target'], pred=results_df['target'])\n",
    "cm = cm_result(true=test_data['target'], pred=results_df['target'])\n",
    "cm_avg = cm_avg_result(true=test_data['target'], pred=results_df['target'])\n",
    "\n",
    "print(acc, f1, cm_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02052a80-7128-47de-8902-06f3d9406a0c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-01T00:15:08.052409Z",
     "iopub.status.idle": "2022-09-01T00:15:08.052409Z",
     "shell.execute_reply": "2022-09-01T00:15:08.052409Z",
     "shell.execute_reply.started": "2022-09-01T00:15:08.052409Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create sweep for parameter tuning with wandb with evaluation and training fucntions\n",
    "# modify function to be used for testing both sdg and target classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
